{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21731e6e-36bb-4cc6-ba62-cca9ee5129df",
   "metadata": {},
   "source": [
    "# Uploading the zarr cube to a AWS S3 bucket for streaming access\n",
    "---\n",
    "Once the cubes are created via the analysis pipeline [./02_cubePipeline.ipynb](./02_cubePipeline.ipynb), the AWS S3 account is created and he AWS access key is added to the .env file as specified in the [README.md](../../README.md), the cubes can be uploaded.\n",
    "\n",
    "1. import the necessary modules and create the .aws folder with a credentials file if not existing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04bc485-87ad-4b4d-bcb0-1bff0951a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores used: 115\n"
     ]
    }
   ],
   "source": [
    "############# Import the necessary modules #############\n",
    "#python basemodules and jupyter modules\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "from dotenv import load_dotenv\n",
    "# get the base path of the repository\n",
    "repo_dir = os.popen('git rev-parse --show-toplevel').read().strip()\n",
    "###load the .env file\n",
    "load_dotenv(dotenv_path=f\"{repo_dir}/.env\")\n",
    "\n",
    "\n",
    "# AWS SDK modules\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "############# Create .aws folder with credentials and config file #############\n",
    "os.chdir(os.path.expanduser(\"~\"))\n",
    "if not os.path.exists(\".aws\"):\n",
    "    os.mkdir(\".aws\")\n",
    "os.chdir(\".aws\")\n",
    "# Create a credentials file with the access key and secret key from the .env file\n",
    "with open(\"credentials\", \"w\") as file:\n",
    "    file.write(f\"[default]\\naws_access_key_id = {os.getenv('AWS_ACCESS_KEY_ID')}\\naws_secret_access_key = {os.getenv('AWS_SECRET_ACCESS_KEY')}\")\n",
    "# Create a config file with the region from the .env file and json as output format\n",
    "with open(\"config\", \"w\") as file:\n",
    "    file.write(f\"[default]\\nregion = {os.getenv('AWS_REGION')}\\noutput = json\")\n",
    "#print(f\"Credentials and config files created in {os.getcwd()}: {os.listdir()}\")\n",
    "#read credentials file\n",
    "# with open(\"credentials\", \"r\") as file:\n",
    "#     print(file.read())\n",
    "\n",
    "############## calculate the number of cores for distributed processing\n",
    "total_cpus = mp.cpu_count() #int(sys.argv[1])\n",
    "n_cores = int(total_cpus * 0.9) // 1\n",
    "print(\"Number of cores used:\", n_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834de61-fe99-4c8c-9230-4003eb8df84a",
   "metadata": {},
   "source": [
    "2. Define the functions for the upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "128b906e-826d-4d47-a09c-fccc0745ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Define the s3 functions #############\n",
    "def bucket_exists(bucket_name):\n",
    "    \"\"\"Check if an S3 bucket with the specified name already exists.\n",
    "\n",
    "    :param bucket_name: Name of the bucket to check\n",
    "    :return: True if the bucket exists, else False\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        # List all buckets\n",
    "        response = s3_client.list_buckets()\n",
    "        # Check if the bucket exists in the list of buckets\n",
    "        for bucket in response['Buckets']:\n",
    "            if bucket['Name'] == bucket_name:\n",
    "                return True\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    \n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Check if the bucket already exists\n",
    "    if bucket_exists(bucket_name):\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "        return False\n",
    "\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region is None:\n",
    "            # If no region is specified, use the default S3 region\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            # Specify the region for the bucket\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "        logging.info(f\"Bucket {bucket_name} created successfully.\")\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def set_bucket_public(bucket_name: str) -> bool:\n",
    "    \"\"\"Set the bucket policy to make all objects in the bucket publicly readable\n",
    "\n",
    "    :param bucket_name: Bucket to set the policy on\n",
    "    :return: True if policy was set, else False\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    public_policy = {\n",
    "        # Specify the version of the policy language\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        # Define the statement section, which is a list of policy statements\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                # Statement ID for identifying this statement, useful for managing policies with multiple statements\n",
    "                \"Sid\": \"AddPublicReadCannedAcl\",\n",
    "                # Effect determines if the action is allowed or denied; here, it allows the action\n",
    "                \"Effect\": \"Allow\",\n",
    "                # Principal specifies who the policy applies to\n",
    "                \"Principal\": {\n",
    "                    # The principals are specified using Amazon Resource Names (ARNs)\n",
    "                    \"AWS\": [\n",
    "                        f\"arn:aws:iam::{os.getenv('AWS_ACCOUNT_ID')}:root\",  # First AWS account root user\n",
    "                        #\"arn:aws:iam::444455556666:root\"   # optional, second or more AWS account root user\n",
    "                    ]\n",
    "                },\n",
    "                # Action specifies the operations that are allowed\n",
    "                \"Action\": [\n",
    "                    \"s3:PutObject\",     # Allows uploading of objects to the specified S3 bucket\n",
    "                    \"s3:PutObjectAcl\"   # Allows setting the ACL of objects in the specified S3 bucket\n",
    "                ],\n",
    "                # Resource specifies the AWS resources the actions apply to\n",
    "                \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\",  # Applies to all objects in the bucket\n",
    "                # Condition specifies the conditions under which the policy is in effect\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        # Condition ensures the ACL is set to public-read when actions are performed\n",
    "                        \"s3:x-amz-acl\": [\n",
    "                            \"public-read\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        s3_client.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(public_policy))#, ExpectedBucketOwner=os.getenv('AWS_ACCOUNT_ID'))\n",
    "        print(f'Bucket policy set to public for {bucket_name}.')\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error setting bucket policy for {bucket_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def upload_directory(directory_name, bucket, s3_prefix=''):\n",
    "    \"\"\"Upload a directory to an S3 bucket\n",
    "\n",
    "    :param directory_name: Directory to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param s3_prefix: S3 prefix for the uploaded files\n",
    "    :return: True if directory was uploaded, else False\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_name):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            s3_path = os.path.relpath(file_path, directory_name)\n",
    "            if s3_prefix:\n",
    "                s3_path = os.path.join(s3_prefix, s3_path)\n",
    "\n",
    "            try:\n",
    "                s3_client.upload_file(file_path, bucket, s3_path)\n",
    "                print(f'Successfully uploaded {file_path} to s3://{bucket}/{s3_path}')\n",
    "            except ClientError as e:\n",
    "                logging.error(e)\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def convert_to_valid_bucket_name(original_name: str) -> str:\n",
    "    # Convert to lowercase\n",
    "    bucket_name = original_name.lower()\n",
    "\n",
    "    # Replace underscores and spaces with hyphens\n",
    "    bucket_name = re.sub(r'[_\\s]+', '-', bucket_name)\n",
    "\n",
    "    # Remove any character that isn't lowercase letter, number, or hyphen\n",
    "    bucket_name = re.sub(r'[^a-z0-9-]', '', bucket_name)\n",
    "\n",
    "    # Ensure the name starts and ends with a letter or number\n",
    "    bucket_name = re.sub(r'(^-|-$)', '', bucket_name)\n",
    "\n",
    "    # Trim the name to 63 characters if too long\n",
    "    bucket_name = bucket_name[:63]\n",
    "\n",
    "    # Ensure the name is at least 3 characters long\n",
    "    if len(bucket_name) < 3:\n",
    "        bucket_name = bucket_name.ljust(3, 'a')  # Pad with 'a' if too short\n",
    "\n",
    "    return bucket_name\n",
    "\n",
    "\n",
    "def get_bucket_memory_usage(bucket_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the total memory usage of an S3 bucket.\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    :return: The total memory usage in bytes.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    total_size = 0\n",
    "    continuation_token = None\n",
    "\n",
    "    while True:\n",
    "        # Use list_objects_v2 to handle large numbers of objects\n",
    "        if continuation_token:\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket_name, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "        # Check if the response contains 'Contents'\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                total_size += obj['Size']\n",
    "\n",
    "        # Check for continuation token to handle paginated responses\n",
    "        if 'NextContinuationToken' in response:\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ad9d6-f1be-49e6-97ef-7110ca6d5a22",
   "metadata": {},
   "source": [
    "3. Retrieve the list of existing buckets and compare the memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4254e475-e3b2-4725-a53c-e3ffc2cfb1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets:\n",
      "cryo-cube-20200704.zarr: 1334.86 MB\n",
      "********************\n",
      "AWS memory limit set to: 5000 MB\n",
      "Memory space used on AWS in MB:1334.864541053772\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "############# Check the current buckets and compare the memory usage  #############\n",
    "# Retrieve the list of existing buckets\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_buckets()\n",
    "memory_limit=int(os.getenv('AWS_MAX_MEMORY_MB', '0'))\n",
    "\n",
    "# Output the bucket names\n",
    "print('Existing buckets:')\n",
    "bucket_memory=[]\n",
    "if response['Buckets']:\n",
    "    for bucket in response['Buckets']:\n",
    "        bucket_name = bucket[\"Name\"]\n",
    "        total_size = get_bucket_memory_usage(bucket_name)\n",
    "        total_size_mb = total_size / (1024 * 1024)  # Convert bytes to megabytes\n",
    "        bucket_memory.append((bucket_name,total_size_mb))\n",
    "        print(f'{bucket_name}: {total_size_mb:.2f} MB')\n",
    "else:\n",
    "    print('No buckets exist')\n",
    "total_memory=sum([x[1] for x in bucket_memory])\n",
    "\n",
    "print(20*\"*\")\n",
    "print(f\"AWS memory limit set to: {memory_limit} MB\")\n",
    "print(f\"Memory space used on AWS in MB:{total_memory}\")\n",
    "print(20*\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021854d2-946e-48c3-bfa1-10f870dc6a57",
   "metadata": {},
   "source": [
    "4. Upload the cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad71d119-57ee-4ea8-9819-974d47914d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cube memory in MB: 1334.8645782470703\n",
      "List of cubes to upload: ['cryo_cube_20200704.zarr']\n",
      "Uploading all the cubes...\n",
      "********************\n",
      "Cube memory in MB: 1334.8645782470703\n",
      "Attempting to upload the cube cryo_cube_20200704.zarr to the new bucket: cryo-cube-20200704.zarr\n",
      "Bucket cryo-cube-20200704.zarr already exists.\n",
      "Failed to create bucket cryo-cube-20200704.zarr.\n",
      "Total uploading time in seconds:0.16809606552124023\n",
      "Average uploading time per file in seconds:0.16815829277038574\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "###########final pipeline fucntions to upload the cubes ###############\n",
    "def get_cube_memory(cube_name):\n",
    "    \"\"\"\n",
    "    Calculate the memory usage of a cube directory.\n",
    "\n",
    "    This function uses the `du` command to get the size of the directory\n",
    "    in bytes, and then converts it to megabytes (MB).\n",
    "\n",
    "    :param cube_name: The name of the cube directory.\n",
    "    :return: The memory usage of the cube in megabytes (MB).\n",
    "    \"\"\"\n",
    "    # Get the size of the directory\n",
    "    result = subprocess.run(['du', '-sb', cube_name], stdout=subprocess.PIPE, text=True)\n",
    "    cube_memory = int(result.stdout.split()[0])\n",
    "    # Turn cube_memory into MB\n",
    "    cube_memory_mb = cube_memory / (1024 * 1024)\n",
    "    print(f\"Cube memory in MB: {cube_memory_mb}\")\n",
    "    return cube_memory_mb\n",
    "    \n",
    "def bucket_pipeline(cube_name):\n",
    "    \"\"\"\n",
    "    Upload a cube to an S3 bucket if within memory limits.\n",
    "\n",
    "    This function checks if the memory usage of a cube is within the \n",
    "    specified memory limit. If so, it creates an S3 bucket, sets it public, \n",
    "    and uploads the cube. It reports success or failure at each step.\n",
    "\n",
    "    :param cube_name: The name of the cube directory to upload.\n",
    "    \"\"\"\n",
    "    cube_memory_mb = get_cube_memory(cube_name)\n",
    "    \n",
    "    bucket=convert_to_valid_bucket_name(cube_name.split(\".zarr\")[0]) + \".zarr\"\n",
    "    print(f\"Attempting to upload the cube {cube_name} to the new bucket: {bucket}\")\n",
    "\n",
    "    if total_memory + cube_memory_mb < int(memory_limit):\n",
    "        # Create the bucket\n",
    "        if create_bucket(bucket, \"eu-north-1\"):\n",
    "            if set_bucket_public(bucket):\n",
    "                if upload_directory(cube_dir, bucket):\n",
    "                    print(f'Successfully uploaded directory {cube_name} to bucket {bucket}.')\n",
    "                else:\n",
    "                    print(f'Failed to upload directory {cube_name} to bucket {bucket}.')\n",
    "            else:\n",
    "                print(f'Failed to set the bucket policy for {bucket}.')\n",
    "        else:\n",
    "            print(f'Failed to create bucket {bucket}.')\n",
    "    else:\n",
    "        print(f\"Bucket {bucket} would exceed the memory limit of {memory_limit} MB. Total memory usage is {total_memory} MB.\")\n",
    "\n",
    "# Define the cube to upload and its memory\n",
    "zarr_base= repo_dir #os.getenv(\"ZARR_BASE_FOLDER\")\n",
    "os.chdir(zarr_base)\n",
    "cubes=glob.glob(\"*.zarr\")\n",
    "\n",
    "total_cube_memory=0\n",
    "cube_index=0\n",
    "while cube_index < len(cubes) and total_cube_memory < memory_limit:\n",
    "    cube_memory = get_cube_memory(cubes[cube_index])\n",
    "    # Check if cube would exceed the memory limit\n",
    "    if total_cube_memory + cube_memory < memory_limit:\n",
    "        total_cube_memory += cube_memory\n",
    "        cube_index += 1\n",
    "    else:\n",
    "        # Stop if memory limit would be exceeded\n",
    "        break\n",
    "cubes_to_upload=cubes[0:cube_index]\n",
    "print(f\"List of cubes to upload: {cubes_to_upload}\")\n",
    "\n",
    "\n",
    "##############multithreaded upload of the cubes\n",
    "print(\"Uploading all the cubes...\")\n",
    "print(20*\"*\")\n",
    "start=time.time()\n",
    "# multithreading the fft calculation\n",
    "with mp.pool.ThreadPool(n_cores) as pool:\n",
    "    pool.starmap(bucket_pipeline, [(cube_name,) for cube_name in cubes_to_upload])\n",
    "print(f\"Total uploading time in seconds:{time.time()-start}\") \n",
    "print(f\"Average uploading time per file in seconds:{(time.time()-start)/len(cubes_to_upload)}\") \n",
    "print(20*\"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331847a0-a683-4658-a5dc-c4d7886719ab",
   "metadata": {},
   "source": [
    "5. In case you want to delete the existing buckets, you can use the following:\n",
    "\n",
    "> [!WARNING]  \n",
    "Only run this if necessary! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3328fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket cryo-cube-20200704.zarr deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "############# Deleting the existing buckets #############\n",
    "def delete_bucket(bucket_name: str):\n",
    "    \"\"\"\n",
    "    Delete the specified S3 bucket, including all its objects.\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "    try:\n",
    "        # Delete all objects in the bucket\n",
    "        bucket.objects.delete()\n",
    "        \n",
    "        # Now delete the bucket\n",
    "        s3_client.delete_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} deleted successfully.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error deleting bucket {bucket_name}: {e}\")\n",
    "\n",
    "### delete existing buckets\n",
    "for bucket in response['Buckets']:\n",
    "    delete_bucket(bucket[\"Name\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rhoneCube",
   "language": "python",
   "name": "rhonecube"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
