{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21731e6e-36bb-4cc6-ba62-cca9ee5129df",
   "metadata": {},
   "source": [
    "# Uploading the zarr cube to a AWS S3 bucket for streaming access\n",
    "---\n",
    "Once the cubes are created via the analysis pipeline [./02_cubePipeline.ipynb](./02_cubePipeline.ipynb), the AWS S3 account is created and he AWS access key is added to the .env file as specified in the [README.md](../../README.md), the cubes can be uploaded.\n",
    "\n",
    "1. import the necessary modules and create the .aws folder with a credentials file if not existing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e04bc485-87ad-4b4d-bcb0-1bff0951a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores used: 115\n",
      "Bucket name: rhone-glacier-das\n",
      "Client created: <botocore.client.S3 object at 0x1472f038a2e0>\n"
     ]
    }
   ],
   "source": [
    "############# Import the necessary modules #############\n",
    "#python basemodules and jupyter modules\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "from dotenv import load_dotenv\n",
    "# get the base path of the repository\n",
    "repo_dir = os.popen('git rev-parse --show-toplevel').read().strip()\n",
    "###load the .env file\n",
    "load_dotenv(dotenv_path=f\"{repo_dir}/.env\")\n",
    "\n",
    "\n",
    "# AWS SDK modules\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "############# Create .aws folder with credentials and config file #############\n",
    "os.chdir(os.path.expanduser(\"~\"))\n",
    "if not os.path.exists(\".aws\"):\n",
    "    os.mkdir(\".aws\")\n",
    "os.chdir(\".aws\")\n",
    "# Create a credentials file with the access key and secret key from the .env file\n",
    "with open(\"credentials\", \"w\") as file:\n",
    "    file.write(f\"[default]\\naws_access_key_id = {os.getenv('AWS_ACCESS_KEY_ID')}\\naws_secret_access_key = {os.getenv('AWS_SECRET_ACCESS_KEY')}\")\n",
    "# Create a config file with the region from the .env file and json as output format\n",
    "with open(\"config\", \"w\") as file:\n",
    "    file.write(f\"[default]\\nregion = {os.getenv('AWS_REGION')}\\noutput = json\")\n",
    "#print(f\"Credentials and config files created in {os.getcwd()}: {os.listdir()}\")\n",
    "#read credentials file\n",
    "# with open(\"credentials\", \"r\") as file:\n",
    "#     print(file.read())\n",
    "\n",
    "############## calculate the number of cores for distributed processing\n",
    "total_cpus = mp.cpu_count() #int(sys.argv[1])\n",
    "n_cores = int(total_cpus * 0.9) // 1\n",
    "print(\"Number of cores used:\", n_cores)\n",
    "\n",
    "############# Get the AWS parameters from .env #############\n",
    "def convert_to_valid_bucket_name(original_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a string to a valid S3 bucket name.\n",
    "    \n",
    "    :param original_name: The original name to convert.\n",
    "    :return: The converted name.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    bucket_name = original_name.lower()\n",
    "\n",
    "    # Replace underscores and spaces with hyphens\n",
    "    bucket_name = re.sub(r'[_\\s]+', '-', bucket_name)\n",
    "\n",
    "    # Remove any character that isn't lowercase letter, number, or hyphen\n",
    "    bucket_name = re.sub(r'[^a-z0-9-]', '', bucket_name)\n",
    "\n",
    "    # Ensure the name starts and ends with a letter or number\n",
    "    bucket_name = re.sub(r'(^-|-$)', '', bucket_name)\n",
    "\n",
    "    # Trim the name to 63 characters if too long\n",
    "    bucket_name = bucket_name[:63]\n",
    "\n",
    "    # Ensure the name is at least 3 characters long\n",
    "    if len(bucket_name) < 3:\n",
    "        bucket_name = bucket_name.ljust(3, 'a')  # Pad with 'a' if too short\n",
    "\n",
    "    return bucket_name\n",
    "\n",
    "# Set the bucket name and memory limit\n",
    "os.chdir(repo_dir)\n",
    "bucket_name = convert_to_valid_bucket_name(os.getenv(\"BUCKET_NAME\", \"rhone-glacier-das\"))\n",
    "memory_limit=int(os.getenv('AWS_MAX_MEMORY_MB', '0'))\n",
    "region=os.getenv('AWS_REGION', 'eu-north-1')\n",
    "print(f\"Bucket name: {bucket_name}\")\n",
    "\n",
    "############# Setup an AWS client #############\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "print(f\"Client created: {s3_client}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834de61-fe99-4c8c-9230-4003eb8df84a",
   "metadata": {},
   "source": [
    "2. Create the bucket if not exists and set its policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "128b906e-826d-4d47-a09c-fccc0745ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Creating bucket rhone-glacier-das in eu-north-1 region.\n",
      "Bucket rhone-glacier-das created successfully.\n",
      "********************\n",
      "Public access block settings updated for bucket rhone-glacier-das.\n",
      "Setting bucket policy for rhone-glacier-das.\n",
      "Bucket policy set successfully for rhone-glacier-das.\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "############# Define the s3 bucket functions #############\n",
    "def bucket_exists(bucket_name: str, s3_client: boto3.client) -> bool:\n",
    "    \"\"\"Check if an S3 bucket with the specified name already exists.\n",
    "\n",
    "    :param bucket_name: Name of the bucket to check\n",
    "    :param s3_client: Boto3 S3 client\n",
    "    :return: True if the bucket exists, else False\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # List all buckets\n",
    "        response = s3_client.list_buckets()\n",
    "        # Check if the bucket exists in the list of buckets\n",
    "        for bucket in response['Buckets']:\n",
    "            if bucket['Name'] == bucket_name:\n",
    "                return True\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    \n",
    "def create_bucket(bucket_name: str, s3_client: boto3.client, region: str) -> bool:\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param s3_client: Boto3 S3 client\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Check if the bucket already exists\n",
    "    if bucket_exists(bucket_name, s3_client):\n",
    "        print(f\"Bucket {bucket_name} already exists. Probable URL: https://s3.{region}.amazonaws.com/{bucket_name}\")\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        # Specify the region for the bucket\n",
    "        location = {'LocationConstraint': region}\n",
    "        s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                CreateBucketConfiguration=location)\n",
    "        print(f\"Bucket {bucket_name} created successfully.\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def update_public_access_block(bucket_name: str, s3_client: boto3.client):\n",
    "    \"\"\"Update the public access block settings for an S3 bucket.\"\"\"\n",
    "    try:\n",
    "        # Define the public access block configuration\n",
    "        public_access_block_config = {\n",
    "            'BlockPublicAcls': False,  # Set to False to allow public ACLs\n",
    "            'IgnorePublicAcls': False,  # Set to False to respect public ACLs\n",
    "            'BlockPublicPolicy': False,  # Set to False to allow public policies\n",
    "            'RestrictPublicBuckets': False  # Set to False to not restrict public access\n",
    "        }\n",
    "\n",
    "        # Apply the public access block configuration to the bucket\n",
    "        s3_client.put_public_access_block(\n",
    "            Bucket=bucket_name,\n",
    "            PublicAccessBlockConfiguration=public_access_block_config\n",
    "        )\n",
    "        print(f\"Public access block settings updated for bucket {bucket_name}.\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error updating public access block settings for bucket {bucket_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def set_bucket_policy(bucket_name: str, s3_client: boto3.client) -> bool:\n",
    "    \"\"\"\n",
    "    Set the bucket policy to allow specified AWS accounts to upload objects and \n",
    "    make all objects in the bucket publicly readable.\n",
    "\n",
    "    :param bucket_name: Bucket to set the policy on\n",
    "    :param s3_client: Boto3 S3 client\n",
    "    :return: True if the policy was set, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the bucket policy\n",
    "    bucket_policy = {\n",
    "        # Specify the version of the policy language\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        # Define the statement section, which is a list of policy statements\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                # Allows everyone to read objects\n",
    "                \"Sid\": \"AllowPublicRead\",  #Statement ID for identifying this statement, useful for managing policies with multiple statements\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": \"*\", #{\"AWS\": [f\"arn:aws:iam::{os.getenv('AWS_ACCOUNT_ID')}:root\",]}, # AWS account id\n",
    "                \"Action\": [\"s3:GetObject\",\"s3:GetObjectAcl\"], # operation\n",
    "                \"Resource\": [f\"arn:aws:s3:::{bucket_name}/*\"], #bucket resource to change\n",
    "                \"Condition\": {\n",
    "                     \"Bool\": {\n",
    "                         \"aws:SecureTransport\":\"true\"\n",
    "                     }\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Apply the bucket policy\n",
    "        s3_client.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(bucket_policy), ExpectedBucketOwner=os.getenv('AWS_ACCOUNT_ID'))\n",
    "        print(f'Bucket policy set successfully for {bucket_name}.')\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error setting bucket policy for {bucket_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# create the bucket\n",
    "print(20*\"*\")\n",
    "print(f\"Creating bucket {bucket_name} in {region} region.\")\n",
    "bucket_exists=create_bucket(bucket_name, s3_client, region)\n",
    "print(20*\"*\")\n",
    "\n",
    "# set the bucket policy\n",
    "if bucket_exists:\n",
    "    if update_public_access_block(bucket_name, s3_client):\n",
    "        print(f\"Setting bucket policy for {bucket_name}.\")\n",
    "        set_bucket_policy(bucket_name, s3_client)\n",
    "    else:\n",
    "        print(f\"Could not set public access block for bucket {bucket_name}\")\n",
    "else:\n",
    "    print(f\"Bucket {bucket_name} does not exist. Policy not set.\")\n",
    "print(20*\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ad9d6-f1be-49e6-97ef-7110ca6d5a22",
   "metadata": {},
   "source": [
    "3. Retrieve the list of existing buckets and compare the memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4254e475-e3b2-4725-a53c-e3ffc2cfb1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets:\n",
      "rhone-glacier-das: 0.00 MB\n",
      "********************\n",
      "AWS memory limit set to: 5000 MB\n",
      "Memory space used on AWS in MB: 0.0\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "############# Check the current buckets and compare the memory usage  #############\n",
    "# define a function to get the memory usage of a bucket\n",
    "def get_bucket_memory_usage(bucket_name: str, s3_client: boto3.client) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the total memory usage of an S3 bucket.\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    :param s3_client: The Boto3 S3 client.\n",
    "    \n",
    "    :return: The total memory usage in bytes.\n",
    "    \"\"\"\n",
    "\n",
    "    total_size = 0\n",
    "    continuation_token = None\n",
    "\n",
    "    while True:\n",
    "        # Use list_objects_v2 to handle large numbers of objects\n",
    "        if continuation_token:\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket_name, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "        # Check if the response contains 'Contents'\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                total_size += obj['Size']\n",
    "\n",
    "        # Check for continuation token to handle paginated responses\n",
    "        if 'NextContinuationToken' in response:\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_size\n",
    "\n",
    "\n",
    "# Output the bucket names\n",
    "response = s3_client.list_buckets()\n",
    "print('Existing buckets:')\n",
    "bucket_memory=[]\n",
    "if response['Buckets']:\n",
    "    for bucket in response['Buckets']:\n",
    "        bucket_name = bucket[\"Name\"]\n",
    "        total_size = get_bucket_memory_usage(bucket_name, s3_client)\n",
    "        total_size_mb = total_size / (1024 * 1024)  # Convert bytes to megabytes\n",
    "        bucket_memory.append((bucket_name,total_size_mb))\n",
    "        print(f'{bucket_name}: {total_size_mb:.2f} MB')\n",
    "else:\n",
    "    print('No buckets exist')\n",
    "total_aws_memory=sum([x[1] for x in bucket_memory])\n",
    "\n",
    "print(20*\"*\")\n",
    "print(f\"AWS memory limit set to: {memory_limit} MB\")\n",
    "print(f\"Memory space used on AWS in MB: {total_aws_memory}\")\n",
    "print(20*\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021854d2-946e-48c3-bfa1-10f870dc6a57",
   "metadata": {},
   "source": [
    "4. Upload the cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad71d119-57ee-4ea8-9819-974d47914d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cube memory in MB: 1334.8645782470703\n",
      "List of cubes to upload: ['cryo_cube_20200704.zarr']\n",
      "Uploading all the cubes...\n",
      "********************\n",
      "Cube memory in MB: 1334.8645782470703\n",
      "Attempting to upload the cube cryo_cube_20200704.zarr to the new bucket: rhone-glacier-das\n",
      "Successfully uploaded cryo_cube_20200704.zarr/.zgroup\n",
      "Successfully uploaded cryo_cube_20200704.zarr/.zattrs\n",
      "Successfully uploaded cryo_cube_20200704.zarr/.zmetadata\n",
      "Successfully uploaded cryo_cube_20200704.zarr/channel/.zarray\n",
      "Successfully uploaded cryo_cube_20200704.zarr/channel/.zattrs\n",
      "Successfully uploaded cryo_cube_20200704.zarr/channel/0\n",
      "Successfully uploaded cryo_cube_20200704.zarr/frequency/.zattrs\n",
      "Successfully uploaded cryo_cube_20200704.zarr/frequency/.zarray\n",
      "Successfully uploaded cryo_cube_20200704.zarr/frequency/0\n",
      "Successfully uploaded cryo_cube_20200704.zarr/cwt/.zattrs\n",
      "Successfully uploaded cryo_cube_20200704.zarr/cwt/.zarray\n",
      "Successfully uploaded cryo_cube_20200704.zarr/cwt/2.0.0\n",
      "Successfully uploaded cryo_cube_20200704.zarr/cwt/0.0.0\n",
      "Successfully uploaded cryo_cube_20200704.zarr/cwt/1.0.0\n",
      "Successfully uploaded cryo_cube_20200704.zarr/time/0\n",
      "Successfully uploaded cryo_cube_20200704.zarr/time/.zattrs\n",
      "Successfully uploaded cryo_cube_20200704.zarr/time/.zarray\n",
      "Successfully uploaded cryo_cube_20200704.zarr/fft/2.0.0\n",
      "Successfully uploaded cryo_cube_20200704.zarr/fft/.zarray\n",
      "Successfully uploaded cryo_cube_20200704.zarr/fft/.zattrs\n",
      "Successfully uploaded cryo_cube_20200704.zarr/fft/0.0.0\n",
      "Successfully uploaded cryo_cube_20200704.zarr/fft/1.0.0\n",
      "Zarr cube available at: https://s3.eu-north-1.amazonaws.com/rhone-glacier-das/cryo_cube_20200704.zarr\n",
      "Successfully uploaded directory cryo_cube_20200704.zarr to bucket {'Name': 'rhone-glacier-das', 'CreationDate': datetime.datetime(2024, 8, 7, 1, 50, 13, tzinfo=tzutc())}.\n",
      "Total uploading time in seconds:9.106251001358032\n",
      "Average uploading time per file in seconds:9.106330156326294\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "###########final pipeline fucntions to upload the cubes ###############\n",
    "def get_cube_memory(cube_name):\n",
    "    \"\"\"\n",
    "    Calculate the memory usage of a cube directory.\n",
    "\n",
    "    This function uses the `du` command to get the size of the directory\n",
    "    in bytes, and then converts it to megabytes (MB).\n",
    "\n",
    "    :param cube_name: The name of the cube directory.\n",
    "    :return: The memory usage of the cube in megabytes (MB).\n",
    "    \"\"\"\n",
    "    # Get the size of the directory\n",
    "    result = subprocess.run(['du', '-sb', cube_name], stdout=subprocess.PIPE, text=True)\n",
    "    cube_memory = int(result.stdout.split()[0])\n",
    "    # Turn cube_memory into MB\n",
    "    cube_memory_mb = cube_memory / (1024 * 1024)\n",
    "    print(f\"Cube memory in MB: {cube_memory_mb}\")\n",
    "    return cube_memory_mb\n",
    "\n",
    "# Define the function to upload a directory to an S3 bucket\n",
    "def upload_directory(directory_name, bucket_name, region, s3_prefix=''):\n",
    "    \"\"\"Upload a directory to an S3 bucket\n",
    "\n",
    "    :param directory_name: Directory to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param s3_prefix: S3 prefix for the uploaded files\n",
    "    :return: True if directory was uploaded, else False\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_name):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            s3_path = os.path.relpath(file_path, os.path.dirname(directory_name)) #set directory root\n",
    "\n",
    "            if s3_prefix:\n",
    "                s3_path = os.path.join(s3_prefix, s3_path)\n",
    "\n",
    "            try:\n",
    "                s3_client.upload_file(file_path, bucket_name ,s3_path) #, ExtraArgs={'ACL':'public-read'})\n",
    "                print(f'Successfully uploaded {file_path}')\n",
    "            except ClientError as e:\n",
    "                logging.error(e)\n",
    "                return False\n",
    "    print(f\"Zarr cube available at: https://s3.{region}.amazonaws.com/{bucket_name}/{directory_name}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "    \n",
    "def upload_pipeline(cube_name, bucket_name, total_aws_memory, memory_limit, region):\n",
    "    \"\"\"\n",
    "    Upload a cube to an S3 bucket if within memory limits.\n",
    "\n",
    "    This function checks if the memory usage of a cube is within the \n",
    "    specified memory limit. If so, it creates an S3 bucket, sets it public, \n",
    "    and uploads the cube. It reports success or failure at each step.\n",
    "\n",
    "    :param cube_name: The name of the cube directory to upload.\n",
    "    :param bucket_name: The name of the S3 bucket to upload to.\n",
    "    :param total_aws_memory: The total memory usage of all S3 buckets.\n",
    "    :param memory_limit: The maximum memory usage allowed for all S3 buckets.\n",
    "    \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    cube_memory_mb = get_cube_memory(cube_name)\n",
    "    \n",
    "    print(f\"Attempting to upload the cube {cube_name} to the new bucket: {bucket_name}\")\n",
    "    \n",
    "\n",
    "    if total_aws_memory + cube_memory_mb < (int(memory_limit)-total_aws_memory):\n",
    "        if upload_directory(cube_name, bucket_name, region=region):\n",
    "            print(f'Successfully uploaded directory {cube_name} to bucket {bucket}.')\n",
    "        else:\n",
    "            print(f'Failed to upload directory {cube_name} to bucket {bucket}.')\n",
    "    else:\n",
    "        print(f\"Bucket {bucket} would exceed the memory limit of {memory_limit} MB. Total memory usage is {total_aws_memory} MB.\")\n",
    "\n",
    "# Define the cube to upload and its memory\n",
    "zarr_base= repo_dir #os.getenv(\"ZARR_BASE_FOLDER\")\n",
    "os.chdir(zarr_base)\n",
    "cubes=glob.glob(\"*.zarr\")\n",
    "\n",
    "total_cube_memory=0\n",
    "cube_index=0\n",
    "while cube_index < len(cubes) and total_cube_memory < (memory_limit-total_aws_memory):\n",
    "    cube_memory = get_cube_memory(cubes[cube_index])\n",
    "    # Check if cube would exceed the memory limit\n",
    "    if total_cube_memory + cube_memory < memory_limit:\n",
    "        total_cube_memory += cube_memory\n",
    "        cube_index += 1\n",
    "    else:\n",
    "        # Stop if memory limit would be exceeded\n",
    "        break\n",
    "cubes_to_upload=cubes[0:cube_index]\n",
    "print(f\"List of cubes to upload: {cubes_to_upload}\")\n",
    "\n",
    "\n",
    "############## multithreaded upload of the cubes ##############\n",
    "print(\"Uploading all the cubes...\")\n",
    "print(20*\"*\")\n",
    "start=time.time()\n",
    "# multithreading the zarr upload\n",
    "with mp.pool.ThreadPool(n_cores) as pool:\n",
    "    pool.starmap(upload_pipeline, [(cube_name, bucket_name, total_aws_memory, memory_limit, region) for cube_name in cubes_to_upload])\n",
    "print(f\"Total uploading time in seconds:{time.time()-start}\") \n",
    "print(f\"Average uploading time per file in seconds:{(time.time()-start)/len(cubes_to_upload)}\") \n",
    "print(20*\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331847a0-a683-4658-a5dc-c4d7886719ab",
   "metadata": {},
   "source": [
    "5. In case you want to delete the existing buckets, you can use the following:\n",
    "\n",
    "> [!WARNING]  \n",
    "Only run this if necessary! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc3328fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket rhone-glacier-das deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "############# Deleting the existing buckets #############\n",
    "def delete_bucket(bucket_name: str):\n",
    "    \"\"\"\n",
    "    Delete the specified S3 bucket, including all its objects.\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "    try:\n",
    "        # Delete all objects in the bucket\n",
    "        bucket.objects.delete()\n",
    "        \n",
    "        # Now delete the bucket\n",
    "        s3_client.delete_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} deleted successfully.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error deleting bucket {bucket_name}: {e}\")\n",
    "\n",
    "### delete existing buckets\n",
    "delete_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0923bb-1fda-48b2-9dec-b70992f1b2e8",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Extra: \n",
    "\n",
    "If you want to manage and create roles, so access could be further restricted, you can use parts from the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b3a5a-e443-4d6b-a844-c3cd08614d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a session using IAM\n",
    "client = boto3.client('iam')\n",
    "\n",
    "def create_iam_user(user_name: str):\n",
    "    try:\n",
    "        response = client.create_user(UserName=user_name)\n",
    "        print(f\"IAM user {user_name} created successfully.\")\n",
    "        return response['User']['UserName']\n",
    "    except ClientError as e:\n",
    "        print(f\"Error creating IAM user {user_name}: {e}\")\n",
    "        return user_name\n",
    "\n",
    "# Create a new IAM user (replace 'new-username' with your desired username)\n",
    "user_name = create_iam_user('rhone-das')\n",
    "\n",
    "# Define the policy document\n",
    "policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"s3:PutBucketPolicy\",\n",
    "            \"Resource\": \"arn:aws:s3:::your-bucket-name\"  # Replace with your bucket name\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert policy to JSON\n",
    "policy_json = json.dumps(policy_document)\n",
    "\n",
    "# Function to create a new policy or get an existing one\n",
    "def get_or_create_policy(policy_name: str):\n",
    "    try:\n",
    "        # Check if the policy already exists\n",
    "        existing_policies = client.list_policies(Scope='Local')['Policies']\n",
    "        for policy in existing_policies:\n",
    "            if policy['PolicyName'] == policy_name:\n",
    "                print(f\"Policy {policy_name} already exists.\")\n",
    "                return policy['Arn']\n",
    "        \n",
    "        # If not existing, create a new policy\n",
    "        response = client.create_policy(\n",
    "            PolicyName=policy_name,\n",
    "            PolicyDocument=policy_json\n",
    "        )\n",
    "        return response['Policy']['Arn']\n",
    "    except ClientError as e:\n",
    "        print(f\"Error with policy operations: {e}\")\n",
    "        return None\n",
    "\n",
    "# Attach policy to a user\n",
    "def attach_policy_to_user(user_name: str, policy_arn: str):\n",
    "    try:\n",
    "        client.attach_user_policy(\n",
    "            UserName=user_name,\n",
    "            PolicyArn=policy_arn\n",
    "        )\n",
    "        print(f\"Policy {policy_arn} attached to user {user_name}.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error attaching policy to user: {e}\")\n",
    "\n",
    "# Detach policy from all entities\n",
    "def detach_policy(policy_arn: str):\n",
    "    try:\n",
    "        # Detach from all users\n",
    "        attached_users = client.list_entities_for_policy(PolicyArn=policy_arn, EntityFilter='User')\n",
    "        for user in attached_users['PolicyUsers']:\n",
    "            client.detach_user_policy(UserName=user['UserName'], PolicyArn=policy_arn)\n",
    "            print(f\"Policy {policy_arn} detached from user {user['UserName']}.\")\n",
    "\n",
    "        # Detach from all groups\n",
    "        attached_groups = client.list_entities_for_policy(PolicyArn=policy_arn, EntityFilter='Group')\n",
    "        for group in attached_groups['PolicyGroups']:\n",
    "            client.detach_group_policy(GroupName=group['GroupName'], PolicyArn=policy_arn)\n",
    "            print(f\"Policy {policy_arn} detached from group {group['GroupName']}.\")\n",
    "\n",
    "        # Detach from all roles\n",
    "        attached_roles = client.list_entities_for_policy(PolicyArn=policy_arn, EntityFilter='Role')\n",
    "        for role in attached_roles['PolicyRoles']:\n",
    "            client.detach_role_policy(RoleName=role['RoleName'], PolicyArn=policy_arn)\n",
    "            print(f\"Policy {policy_arn} detached from role {role['RoleName']}.\")\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error detaching policy {policy_arn}: {e}\")\n",
    "\n",
    "# Delete policy\n",
    "def delete_policy(policy_arn: str):\n",
    "    try:\n",
    "        client.delete_policy(PolicyArn=policy_arn)\n",
    "        print(f\"Policy {policy_arn} deleted successfully.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error deleting policy {policy_arn}: {e}\")\n",
    "\n",
    "# Get or create the policy\n",
    "policy_arn = get_or_create_policy('AllowS3PutBucketPolicy')\n",
    "\n",
    "# Detach and delete the policy if it exists\n",
    "if policy_arn:\n",
    "    detach_policy(policy_arn)\n",
    "    delete_policy(policy_arn)\n",
    "\n",
    "# Optionally recreate and attach the policy\n",
    "policy_arn = get_or_create_policy('AllowS3PutBucketPolicy')\n",
    "if policy_arn:\n",
    "    attach_policy_to_user(user_name, policy_arn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rhoneCube",
   "language": "python",
   "name": "rhonecube"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
