{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57160f68-b5c7-45d6-8cc5-6f8503bbf6d2",
   "metadata": {},
   "source": [
    "# Benchmark different file transform approaches\n",
    "\n",
    "This document serves the purpose of improving certain time critical parts of the file creat_cube.py from the CryoCube repository on Github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d3fa01-d14d-4cf8-829c-aa8e6fb3c197",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Importing the necessary modules (run poetry install to use the environment for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c20b242-a79a-4dcf-baaf-33fb45c374c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python basemodules and jupyter modules\n",
    "import os\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "from IPython.display import display\n",
    "\n",
    "#benchmarking\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "# data handling\n",
    "import h5py\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import signal, fft\n",
    "import dask.array as da\n",
    "import pyfftw\n",
    "import pyfftw.interfaces.dask_fft as dafft\n",
    "import pickle\n",
    "\n",
    "repo_dir = os.popen('git rev-parse --show-toplevel').read().strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab2777-2e44-43aa-bc85-83bcb0b7e2a0",
   "metadata": {},
   "source": [
    "Reading the folders and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5a57c1-4c99-46d3-99ae-8db0c29583ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "base=\"/work/le837wmue-Rhone_download/DAS_2020\"\n",
    "os.chdir(base)\n",
    "folders=os.listdir()\n",
    "#folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6619016f-a7ba-4783-84c2-dcd2e9e92d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in folder 1: 1307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rhone1khz_UTC_20200707_205138.931.h5',\n",
       " 'rhone1khz_UTC_20200707_154908.931.h5',\n",
       " 'rhone1khz_UTC_20200707_164408.931.h5',\n",
       " 'rhone1khz_UTC_20200707_154608.931.h5',\n",
       " 'rhone1khz_UTC_20200707_235338.931.h5',\n",
       " 'rhone1khz_UTC_20200707_135038.931.h5',\n",
       " 'rhone1khz_UTC_20200707_190438.931.h5',\n",
       " 'rhone1khz_UTC_20200707_182508.931.h5',\n",
       " 'rhone1khz_UTC_20200707_183838.931.h5',\n",
       " 'rhone1khz_UTC_20200707_191908.931.h5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(f\"{base}/{folders[0]}\")\n",
    "files=os.listdir()\n",
    "print(\"Number of files in folder 1:\", len(files))\n",
    "files[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057255b-e7a4-461a-b451-75877e18744e",
   "metadata": {},
   "source": [
    "### Benchmarking the file read and conversion to numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68941c39-36fb-4fe1-a94d-3e3cbb195a91",
   "metadata": {},
   "source": [
    "1. Using h5py as originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb50e97-a1ca-441b-ab94-ba2059d816a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2979  -4623  -8582 ...   3592   2945   3725]\n",
      " [  7908  -2019  -6477 ...  -2881  -5727    647]\n",
      " [-10515  -3669   2995 ...   2528   1463  -5090]\n",
      " ...\n",
      " [  6664  15762  12302 ...  -4615  -8789  -5392]\n",
      " [   804    389   1578 ...   -908 -12766 -12201]\n",
      " [ -4772 -10940  -1687 ...   5943  -1315  -1256]]\n",
      "Time elapsed: 0.4123396873474121\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "f=h5py.File(files[0],  'r')\n",
    "dset=f['Acoustic']\n",
    "print(np.array(dset))\n",
    "end=time.time()\n",
    "print(\"Time elapsed:\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c7478-16cc-4094-8b09-83f987aed9b9",
   "metadata": {},
   "source": [
    "2. Using xarray. Xarray uses distributed chunked reading, so we can assume that it is faster than h5py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b55ed18-13f7-44fe-809e-cf24172ea637",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2979  -4623  -8582 ...   3592   2945   3725]\n",
      " [  7908  -2019  -6477 ...  -2881  -5727    647]\n",
      " [-10515  -3669   2995 ...   2528   1463  -5090]\n",
      " ...\n",
      " [  6664  15762  12302 ...  -4615  -8789  -5392]\n",
      " [   804    389   1578 ...   -908 -12766 -12201]\n",
      " [ -4772 -10940  -1687 ...   5943  -1315  -1256]]\n",
      "Time elapsed: 0.4220571517944336\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "xr_h5=xr.open_dataset(files[0], engine='h5netcdf', backend_kwargs={'phony_dims': 'access'}) # we need to pass phony_dims as the file has no xarray readable dimensions\n",
    "print(xr_h5[\"Acoustic\"].compute().values)\n",
    "end=time.time()\n",
    "print(\"Time elapsed:\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61f8d7-31ec-43a2-b0b0-5eac1e7d31c6",
   "metadata": {},
   "source": [
    "Reading a single file, xarray is about 3 hundredths faster than h5py. Let's see if this scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12cbc7be-0364-4242-88dc-bb0eaa86bcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 20.112152338027954\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "for index,file in enumerate(files[0:40]):\n",
    "    f=h5py.File(files[index],  'r')\n",
    "    dset=f['Acoustic']\n",
    "    np.array(dset)\n",
    "end=time.time()\n",
    "print(\"Time elapsed:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "399d48f3-594b-48e8-be0a-f978678fa6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 3.881666660308838\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "for index,file in enumerate(files[0:40]):\n",
    "    xr_h5=xr.open_dataset(files[index], engine='h5netcdf', backend_kwargs={'phony_dims': 'access'})\n",
    "    xr_h5[\"Acoustic\"].compute().values\n",
    "end=time.time()\n",
    "print(\"Time elapsed:\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf2dcb-63e7-462a-9fdf-157819c9be15",
   "metadata": {},
   "source": [
    "It does! Reading 40 files with h5py takes 17.9 seconds, with xarray it takes 2.9 seconds.\n",
    "Can we use multiprocessing to speed up the process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e22dc4e-7002-4fac-b615-2727c2bdb31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count=mp.cpu_count()*2//3 # we tae two thirds so the open file limit is not exceeded\n",
    "cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4749047-9336-4c0f-ae22-535a47e115f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 3.781881093978882\n"
     ]
    }
   ],
   "source": [
    "def read_file(file):\n",
    "    with h5py.File(file, 'r') as f: # we need with so it actually closes\n",
    "        dset = f['Acoustic']\n",
    "        np.array(dset)\n",
    "\n",
    "start=time.time()\n",
    "pool=mp.Pool(cpu_count)\n",
    "pool.map(read_file, files[0:40])\n",
    "pool.close()\n",
    "pool.join()\n",
    "end=time.time()\n",
    "print(\"Time elapsed:\", end-start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "081937d5-848b-48df-bb0a-45ba890fc9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 2.9076411724090576\n"
     ]
    }
   ],
   "source": [
    "def read_file(file):\n",
    "    xr_h5=xr.open_dataset(file, engine='h5netcdf', backend_kwargs={'phony_dims': 'access'})\n",
    "    xr_h5[\"Acoustic\"].compute().values\n",
    "\n",
    "start=time.time()\n",
    "pool=mp.Pool(cpu_count)\n",
    "pool.map(read_file, files[0:40])\n",
    "pool.close()\n",
    "pool.join()\n",
    "end=time.time()\n",
    "print(\"Time elapsed:\", end-start) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1e76a-3776-41ac-90f0-3c6ba15d5697",
   "metadata": {},
   "source": [
    "Xarray with the underlying dask is already using distributed computing. Still, we see that we can improve the reading and conversion to 1.78 seconds. \n",
    "However, h5py's conversion time is also highly reduced to only 2.54 seconds. Let's try it with more files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9fe5a6-8052-487b-b5e4-2857531e8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with h5py.File(file, 'r') as f: # we need with so it actually closes\n",
    "        dset = f['Acoustic']\n",
    "        np.array(dset)\n",
    "\n",
    "start=time.time()\n",
    "pool=mp.Pool(mp.cpu_count())\n",
    "pool.map(read_file, files[0:200])\n",
    "pool.close()\n",
    "pool.join()\n",
    "end=time.time()\n",
    "print(\"Time elapsed:\", end-start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deabc6e-765c-4172-ad2e-eaf4d6a979ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    xr_h5=xr.open_dataset(file, engine='h5netcdf', backend_kwargs={'phony_dims': 'access'})\n",
    "    xr_h5[\"Acoustic\"].compute().values\n",
    "\n",
    "start=time.time()\n",
    "pool=mp.Pool(cpu_count)\n",
    "pool.map(read_file, files[0:200])\n",
    "pool.close()\n",
    "pool.join()\n",
    "end=time.time()\n",
    "print(\"Time elapsed:\", end-start) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581d56a-4b41-482f-86d3-a5226a87aa1d",
   "metadata": {},
   "source": [
    "As we can see, this scales:\n",
    "200 files with xarray still take only 17.2 seconds, but 200 files with h5py on 85 cpus never finish and the kernel crashes, which might be due to files not being closed properly and using too much memory space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d7a58b-77d7-4408-addf-feccd6fdbcf2",
   "metadata": {},
   "source": [
    "### Fourier transfrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22aee216-9f2f-4cb1-810e-f69edcb696a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########Base settings#########\n",
    "#granularity of spectrogram\n",
    "d_f = 1 # frequency resolution in Hz\n",
    "d_t = 0.1 # time res in seconds\n",
    "\n",
    "# section\n",
    "loc_a, loc_e = 0, 9200 # cable section to be processed (in meters) - 0==start\n",
    "ind_a, ind_e= loc_a//4, loc_e//4 # channel distances (4m each)\n",
    "nFiles = 5 # number of h5 files processed\n",
    "nCores = 8 # cpu cores\n",
    "\n",
    "\n",
    "day= 22\n",
    "month=7\n",
    "sec=0\n",
    "minut=0\n",
    "hours=0\n",
    "\n",
    "# Additional parameters:\n",
    "file_length = 30 # Length of a single h5 file in seconds.\n",
    "NU = 1000 #Sampling frequency in Hz of the recorded data.\n",
    "freq_max = 100 # maximum frequency cut off value for the analysis\n",
    "seg_length=1/d_f #calculate window length corresponding to d_f\n",
    "N = file_length*NU #number of samples in one file\n",
    "ind_f = int(seg_length*freq_max+1)\n",
    "seg_len=int(seg_length*NU) #how many time points should be in one processing window\n",
    "nseg=int(2*(file_length/seg_length)) #amount of segments for the desired window length\n",
    "location_coords = np.arange(loc_a, loc_e, 4)\n",
    "freq_coords=scipy.fft.rfftfreq(int(NU/d_f), 1/NU)[:ind_f]\n",
    "hop = int(d_t*NU)\n",
    "\n",
    "#fft input arguments\n",
    "args = {\n",
    "    \"ind_f\" : ind_f,\n",
    "    \"ind_a\" : ind_a,\n",
    "    \"ind_e\" : ind_e,\n",
    "    \"seg_len\" : seg_len,\n",
    "    \"hop\" : hop,\n",
    "    \"N\" : N\n",
    "}\n",
    "\n",
    "\n",
    "#path and name of resulting zarr-formatted data cube.\n",
    "ZARR_NAME = \"cryo_cube.zarr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba61694-39c4-40fa-8065-c252e34797af",
   "metadata": {},
   "source": [
    "1. The original approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a374772-3c8a-4d9d-9f45-a4ec39901a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to concatenate: 0.03639101982116699\n",
      "Time elapsed for numpy fft: 18.33467960357666\n"
     ]
    }
   ],
   "source": [
    "def channel_fourier_numpy(data, args, taper, positions):\n",
    "    \"\"\"\n",
    "    Applies Fourier Transformation to segments of DAS records to compute spectrograms.\n",
    "\n",
    "    Args:\n",
    "        data (ndarray): The raw data from DAS channels.\n",
    "        args (dict): Contains parameters for Fourier Transform such as segment length and indices.\n",
    "        taper (ndarray): The taper function to apply before the Fourier transform.\n",
    "        positions (ndarray): The positions of the segments.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A 3D array containing the Fourier transform for each segment and channel.\n",
    "    \"\"\"\n",
    "    seg_len = args[\"seg_len\"]\n",
    "    ind_e, ind_a = args[\"ind_e\"], args[\"ind_a\"]\n",
    "    ind_f = args[\"ind_f\"]\n",
    "\n",
    "\n",
    "\n",
    "    # data transformation\n",
    "    segs = ([data[pos:pos+seg_len] for pos in positions]) #dividing the data into segments each consisting of desired amount of data points\n",
    "    segs = [seg.T[ind_a:ind_e] for seg in segs] #transposing the segments individually to gain time series for each channel\n",
    "    nseg = positions.shape[0]\n",
    "    \n",
    "    # the first loop iterates over all segments (each corresponding to a time point)\n",
    "    # in the second loop, the fourier transform gets applied on each channel\n",
    "    Fsegs=np.zeros((nseg, ind_e-ind_a, ind_f))\n",
    "    for i in range(nseg):\n",
    "        for channel_number, channel in enumerate(segs[i]):\n",
    "\n",
    "            # note that modified_log(x)=10*log(x) (conversion to\n",
    "\n",
    "            fourier_transformed = np.fft.rfft(taper*channel, n=seg_len)\n",
    "            fourier_transformed = ((10*np.log(np.abs(fourier_transformed)**2)))[0:ind_f]\n",
    "            fourier_transformed[0]=0\n",
    "            Fsegs[i][channel_number]=fourier_transformed\n",
    "\n",
    "    return Fsegs\n",
    "\n",
    "\n",
    "####### running for only \n",
    "file_index=0\n",
    "f=h5py.File(files[file_index],  'r')\n",
    "dset=f['Acoustic']\n",
    "seg_len=args[\"seg_len\"]\n",
    "hop=args[\"hop\"]\n",
    "N=args[\"N\"]\n",
    "data = np.array(dset) # DAS data\n",
    "taper = signal.windows.tukey(seg_len, 0.25) #taper function - reduce the amplitude of the discontinuities at the boundaries, thereby reducing spectral leakage.\n",
    "\n",
    "\n",
    "# the windowing function (Tukey window in this case) tapers at the ends, \n",
    "#so to avoid losing data at the ends of each file, \n",
    "# the end of one file is overlapped with the beginning of the next file.\n",
    "if file_index!=nFiles-1:\n",
    "\n",
    "    g = h5py.File(files[file_index+1],'r')\n",
    "    dset2=g['Acoustic']\n",
    "    data2= np.array(dset2)\n",
    "    \n",
    "    start=time.time()\n",
    "    data = np.concatenate((data, data2[0:seg_len]), axis=0)\n",
    "    end=time.time()\n",
    "    print(\"Time elapsed to concatenate:\", end-start) \n",
    "\n",
    "j = file_index+1\n",
    "file_pos = file_index * N\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "# If the current file is not the last one\n",
    "if file_index != nFiles-1:\n",
    "    # Calculate the starting positions of each segment in the data\n",
    "    # first segment: (j-1)*N/hop, rounded up\n",
    "    # last segment: (j*N-1)/hop, rounded down\n",
    "    positions = np.arange(np.ceil((j-1)*N/hop), np.floor((j*N-1)/hop)+1, dtype=int)*hop - file_pos # scaled by the hop size and offset by the file position\n",
    "else:\n",
    "    # If last one, start: (j*N-seg_len)/hop\n",
    "    # to ensure that the last segment doesn't extend beyond the end of the data\n",
    "    positions = np.arange(np.ceil((j-1)*N/hop), np.floor((j*N-seg_len)/hop)+1, dtype=int)*hop - file_pos\n",
    "    \n",
    "Fsegs = channel_fourier_numpy(data, args, taper, positions)\n",
    "\n",
    "end=time.time()\n",
    "print(\"Time elapsed for numpy fft:\", end-start) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17b266-08e4-4800-817a-2ea0a2f23195",
   "metadata": {},
   "source": [
    "2. Benchmarking SciPy NumPy pyFFTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d2442fd-f321-46b1-bab9-80c5d3dd7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_fourier(data, args, taper, positions, method='numpy'):\n",
    "    \"\"\"\n",
    "    Applies Fourier Transformation to segments of DAS records using specified method.\n",
    "\n",
    "    Args:\n",
    "        data (ndarray): The raw data from DAS channels.\n",
    "        args (dict): Contains parameters for Fourier Transform such as segment length and indices.\n",
    "        taper (ndarray): The taper function to apply before the Fourier transform.\n",
    "        positions (ndarray): The positions of the segments.\n",
    "        method (str): Method for FFT computation ('numpy', 'scipy', 'pyfftw'). Default is 'numpy'.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A 3D array containing the Fourier transform for each segment and channel.\n",
    "    \"\"\"\n",
    "    seg_len = args[\"seg_len\"]\n",
    "    ind_e, ind_a = args[\"ind_e\"], args[\"ind_a\"]\n",
    "    ind_f = args[\"ind_f\"]\n",
    "\n",
    "    segs = ([data[pos:pos+seg_len] for pos in positions])\n",
    "    segs = [seg.T[ind_a:ind_e] for seg in segs]\n",
    "\n",
    "    nseg = positions.shape[0]\n",
    "    Fsegs = np.zeros((nseg, ind_e-ind_a, ind_f))\n",
    "\n",
    "    if method == 'numpy':\n",
    "        for i in range(nseg):\n",
    "            for channel_number, channel in enumerate(segs[i]):\n",
    "                fourier_transformed = np.fft.rfft(taper * channel, n=seg_len)\n",
    "                fourier_transformed = ((10 * np.log(np.abs(fourier_transformed) ** 2)))[0:ind_f]\n",
    "                fourier_transformed[0] = 0\n",
    "                Fsegs[i][channel_number] = fourier_transformed\n",
    "\n",
    "    elif method == 'scipy':\n",
    "        for i in range(nseg):\n",
    "            for channel_number, channel in enumerate(segs[i]):\n",
    "                fourier_transformed = fft.fft(taper * channel, n=seg_len)\n",
    "                fourier_transformed = ((10 * np.log(np.abs(fourier_transformed) ** 2)))[0:ind_f]\n",
    "                fourier_transformed[0] = 0\n",
    "                Fsegs[i][channel_number] = fourier_transformed\n",
    "\n",
    "\n",
    "    elif method == 'pyfftw':\n",
    "        \n",
    "        try:\n",
    "            with open(f'{repo_dir}/code/notebooks/fftw_wisdom.pkl', 'rb') as f:\n",
    "                wisdom = pickle.load(f)\n",
    "                pyfftw.import_wisdom(wisdom)\n",
    "                print(\"Found a wisdom file.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No wisdom file found. Starting without wisdom.\")\n",
    "\n",
    "        # Pre-allocate the input and output arrays for FFTW\n",
    "        fft_input = pyfftw.empty_aligned(seg_len, dtype='complex128')\n",
    "        fft_output = pyfftw.empty_aligned(seg_len, dtype='complex128')\n",
    "\n",
    "        # Create FFTW object\n",
    "        fft_object = pyfftw.FFTW(fft_input, fft_output)\n",
    "\n",
    "        for i in range(nseg):\n",
    "            for channel_number, channel in enumerate(segs[i]):\n",
    "                fft_input[:] = taper * channel  # Apply taper\n",
    "                fft_object()  # Execute FFT\n",
    "                fourier_transformed = ((10 * np.log(np.abs(fft_output) ** 2)))[0:ind_f]\n",
    "                fourier_transformed[0] = 0\n",
    "                Fsegs[i][channel_number] = fourier_transformed\n",
    "\n",
    "        with open(f'{repo_dir}/code/notebooks/fftw_wisdom.pkl', 'wb') as f:\n",
    "            pickle.dump(pyfftw.export_wisdom(), f)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method specified. Choose from 'numpy', 'scipy', or 'pyfftw'.\")\n",
    "\n",
    "    return Fsegs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1496762-e052-43e1-82de-7975cb16d3b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage for benchmarking\n",
    "file_index = 0\n",
    "seg_len = args[\"seg_len\"]\n",
    "hop = args[\"hop\"]\n",
    "N = args[\"N\"]\n",
    "\n",
    "f = h5py.File(files[file_index], 'r')\n",
    "dset = f['Acoustic']\n",
    "data = np.array(dset)\n",
    "\n",
    "taper = signal.windows.tukey(seg_len, 0.25)\n",
    "\n",
    "if file_index != nFiles - 1:\n",
    "    g = h5py.File(files[file_index + 1], 'r')\n",
    "    dset2 = g['Acoustic']\n",
    "    data2 = np.array(dset2)\n",
    "    data = np.concatenate((data, data2[0:seg_len]), axis=0)\n",
    "\n",
    "j = file_index + 1\n",
    "file_pos = file_index * N\n",
    "\n",
    "if file_index != nFiles - 1:\n",
    "    positions = np.arange(np.ceil((j - 1) * N / hop), np.floor((j * N - 1) / hop) + 1, dtype=int) * hop - file_pos\n",
    "else:\n",
    "    positions = np.arange(np.ceil((j - 1) * N / hop), np.floor((j * N - seg_len) / hop) + 1, dtype=int) * hop - file_pos\n",
    "\n",
    "# Benchmarking each method\n",
    "methods = ['numpy', 'scipy', 'pyfftw']\n",
    "#methods = ['pyfftw']\n",
    "#methods = ['scipy']\n",
    "# methods = ['numpy']\n",
    "for method in methods:\n",
    "    start = time.time()\n",
    "    Fsegs = channel_fourier(data, args, taper, positions, method=method)\n",
    "    end = time.time()\n",
    "    print(f\"Time elapsed for {method} fft:\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30f57f31-8777-4b29-9ca6-05dc319d8a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking method: pyfftw\n",
      "No wisdom file found. Starting without wisdom.\n",
      "Time elapsed for pyfftw fft in file 0: 29.353445529937744\n",
      "Found a wisdom file.\n",
      "Time elapsed for pyfftw fft in file 1: 29.174957036972046\n",
      "Found a wisdom file.\n",
      "Time elapsed for pyfftw fft in file 2: 29.14138913154602\n",
      "Found a wisdom file.\n",
      "Time elapsed for pyfftw fft in file 3: 29.096455097198486\n",
      "Found a wisdom file.\n",
      "Time elapsed for pyfftw fft in file 4: 28.372843265533447\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'files' is a list of file paths and 'nFiles' is the total number of files\n",
    "nFiles = 5  # Set this to the actual number of files you want to process\n",
    "#methods = ['numpy', 'scipy', 'pyfftw']  # FFT methods to benchmark\n",
    "methods = ['pyfftw']\n",
    "#methods = ['scipy']\n",
    "# methods = ['numpy']\n",
    "\n",
    "# Loop over each method first\n",
    "for method in methods:\n",
    "    print(f\"Benchmarking method: {method}\")\n",
    "    # Then loop over each file for the current method\n",
    "    for file_index in range(nFiles):\n",
    "        seg_len = args[\"seg_len\"]\n",
    "        hop = args[\"hop\"]\n",
    "        N = args[\"N\"]\n",
    "\n",
    "        f = h5py.File(files[file_index], 'r')\n",
    "        dset = f['Acoustic']\n",
    "        data = np.array(dset)\n",
    "\n",
    "        taper = signal.windows.tukey(seg_len, 0.25)\n",
    "\n",
    "        if file_index != nFiles - 1:\n",
    "            g = h5py.File(files[file_index + 1], 'r')\n",
    "            dset2 = g['Acoustic']\n",
    "            data2 = np.array(dset2)\n",
    "            data = np.concatenate((data, data2[0:seg_len]), axis=0)\n",
    "\n",
    "        j = file_index + 1\n",
    "        file_pos = file_index * N\n",
    "\n",
    "        if file_index != nFiles - 1:\n",
    "            positions = np.arange(np.ceil((j - 1) * N / hop), np.floor((j * N - 1) / hop) + 1, dtype=int) * hop - file_pos\n",
    "        else:\n",
    "            positions = np.arange(np.ceil((j - 1) * N / hop), np.floor((j * N - seg_len) / hop) + 1, dtype=int) * hop - file_pos\n",
    "\n",
    "        # Benchmark the current method for the current file\n",
    "        start = time.time()\n",
    "        Fsegs = channel_fourier(data, args, taper, positions, method=method)\n",
    "        end = time.time()\n",
    "        print(f\"Time elapsed for {method} fft in file {file_index}:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcca4d3-25af-4f74-b9bd-3e075ff19b22",
   "metadata": {},
   "source": [
    "### Optimizing pyFFTW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89adcf76-1d03-41c5-98a8-b57bd87a0829",
   "metadata": {},
   "source": [
    "#### Using rfft from the builders module of pyFFTW\n",
    "With the simple setup above, pyFFTW and scipy.fft both performed worse than the numpy implementation. However, there is room for improvement of pyFFTW, which is given the name by the original algorithm Fastest Fourier Transform in the West from the corresponding researchers at MIT.\n",
    "\n",
    "Originally RFFT was applied via numpy, we can do the same with the builders model from pyFFTW (a c wrapper for FFTW in python).\n",
    "The \"FFTW_ESTIMATE\" flag estimates the best method instead of measuring it - this can bring some performance. In our case however, it doesnt and for more accuracy we take the measured default. Also confguring multithreading with pyfftw.config.NUM_THREADS = 8 actually leads to about a second worse performance than the standard implementation with a single thread. Suprisingly, also the suage of a wsidom file for the FFTW object creation slows the process by 0.5 seconds, so we use the most simple default setup.\n",
    "Overall we see an improvement of 3 seconds to the original numpy function per file. This of course scales when using a total of 1300 files per folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ffbf9c-9dee-4f33-b7da-aecef0c05f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_fourier(data, args, taper, positions):\n",
    "    \"\"\"\n",
    "    Applies Fourier Transformation to segments of DAS records using specified method.\n",
    "\n",
    "    Args:\n",
    "        data (ndarray): The raw data from DAS channels.\n",
    "        args (dict): Contains parameters for Fourier Transform such as segment length and indices.\n",
    "        taper (ndarray): The taper function to apply before the Fourier transform.\n",
    "        positions (ndarray): The positions of the segments.\n",
    "        method (str): Method for FFT computation ('numpy', 'scipy', 'pyfftw'). Default is 'numpy'.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A 3D array containing the Fourier transform for each segment and channel.\n",
    "    \"\"\"\n",
    "    start=time.time()\n",
    "    seg_len = args[\"seg_len\"]\n",
    "    ind_e, ind_a = args[\"ind_e\"], args[\"ind_a\"]\n",
    "    ind_f = args[\"ind_f\"]\n",
    "\n",
    "    segs = ([data[pos:pos+seg_len] for pos in positions])\n",
    "    segs = [seg.T[ind_a:ind_e] for seg in segs]\n",
    "\n",
    "    nseg = positions.shape[0]\n",
    "    Fsegs = np.zeros((nseg, ind_e-ind_a, ind_f))\n",
    "    step1=time.time()\n",
    "    print(\"Time to create segments: \", step1-start)\n",
    "\n",
    "    # Pre-allocate the input array for FFTW\n",
    "    fft_input = pyfftw.empty_aligned(seg_len, dtype='float64')\n",
    "\n",
    "    # Create FFTW object\n",
    "    fft_object = pyfftw.builders.rfft(fft_input)#, planner_effort='FFTW_ESTIMATE') #, threads=mp.cpu_count()//2)\n",
    "\n",
    "    for i in range(nseg):\n",
    "        for channel_number, channel in enumerate(segs[i]):\n",
    "            fft_input[:] = taper * channel  # Apply taper\n",
    "            fft_output = fft_object()  # Execute FFT\n",
    "            fourier_transformed = ((10 * np.log(np.abs(fft_output) ** 2)))[0:ind_f] # Compute power spectrum\n",
    "            fourier_transformed[0] = 0 # Remove DC component (average value of the signal)\n",
    "            Fsegs[i][channel_number] = fourier_transformed\n",
    "\n",
    "    return Fsegs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3191bb4-17ba-4ae3-ac39-ddc144351e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking method: pyfftw\n",
      "Time to create segments:  0.00024628639221191406\n",
      "Time elapsed for pyfftw rfft in file 0: 15.545733451843262\n",
      "Time to create segments:  0.00023055076599121094\n",
      "Time elapsed for pyfftw rfft in file 1: 15.783392429351807\n",
      "Time to create segments:  0.0002448558807373047\n",
      "Time elapsed for pyfftw rfft in file 2: 15.748984575271606\n",
      "Time to create segments:  0.0002193450927734375\n",
      "Time elapsed for pyfftw rfft in file 3: 15.702316284179688\n",
      "Time to create segments:  0.0002334117889404297\n",
      "Time elapsed for pyfftw rfft in file 4: 15.187469244003296\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'files' is a list of file paths and 'nFiles' is the total number of files\n",
    "nFiles = 5  # Set this to the actual number of files you want to process\n",
    "#methods = ['numpy', 'scipy', 'pyfftw']  # FFT methods to benchmark\n",
    "methods = ['pyfftw']\n",
    "#methods = ['scipy']\n",
    "# methods = ['numpy']\n",
    "\n",
    "# try:\n",
    "#     with open(f'{repo_dir}/code/notebooks/fftw_wisdom.pkl', 'rb') as f:\n",
    "#         wisdom = pickle.load(f)\n",
    "#         pyfftw.import_wisdom(wisdom)\n",
    "#         print(\"Found a wisdom file.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"No wisdom file found. Starting without wisdom.\")\n",
    "\n",
    "# Loop over each method first\n",
    "for method in methods:\n",
    "    print(f\"Benchmarking method: {method}\")\n",
    "    # Then loop over each file for the current method\n",
    "    for file_index in range(nFiles):\n",
    "        seg_len = args[\"seg_len\"]\n",
    "        hop = args[\"hop\"]\n",
    "        N = args[\"N\"]\n",
    "\n",
    "        f = h5py.File(files[file_index], 'r')\n",
    "        dset = f['Acoustic']\n",
    "        data = np.array(dset)\n",
    "\n",
    "        taper = signal.windows.tukey(seg_len, 0.25)\n",
    "\n",
    "        if file_index != nFiles - 1:\n",
    "            g = h5py.File(files[file_index + 1], 'r')\n",
    "            dset2 = g['Acoustic']\n",
    "            data2 = np.array(dset2)\n",
    "            data = np.concatenate((data, data2[0:seg_len]), axis=0)\n",
    "\n",
    "        j = file_index + 1\n",
    "        file_pos = file_index * N\n",
    "\n",
    "        if file_index != nFiles - 1:\n",
    "            positions = np.arange(np.ceil((j - 1) * N / hop), np.floor((j * N - 1) / hop) + 1, dtype=int) * hop - file_pos\n",
    "        else:\n",
    "            positions = np.arange(np.ceil((j - 1) * N / hop), np.floor((j * N - seg_len) / hop) + 1, dtype=int) * hop - file_pos\n",
    "\n",
    "        # Benchmark the current method for the current file\n",
    "        start = time.time()\n",
    "        Fsegs = channel_fourier(data, args, taper, positions, method=method)\n",
    "        end = time.time()\n",
    "        print(f\"Time elapsed for {method} rfft in file {file_index}:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36311b5f-e33d-4054-bd60-7261e219e9a9",
   "metadata": {},
   "source": [
    "#### Using dask via pyFFTW\n",
    "This is a very ineffciient implementation of dask to calculate the fft. The calculation therefore takes minutes to complete. There might however be more promising setups with Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81deecc6-3b2b-4334-8dd4-ea7cc8a4a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_fourier(data, args, taper, positions):\n",
    "    \n",
    "    # Enable the pyfftw cache\n",
    "    pyfftw.interfaces.cache.enable()\n",
    "    start=time.time()\n",
    "    seg_len = args[\"seg_len\"]\n",
    "    ind_e, ind_a = args[\"ind_e\"], args[\"ind_a\"]\n",
    "    ind_f = args[\"ind_f\"]\n",
    "\n",
    "    segs = ([data[pos:pos+seg_len] for pos in positions])\n",
    "    segs = [seg.T[ind_a:ind_e] for seg in segs]\n",
    "\n",
    "    nseg = positions.shape[0]\n",
    "    Fsegs = np.zeros((nseg, ind_e-ind_a, ind_f))\n",
    "    step1=time.time()\n",
    "    print(\"Time to create segments: \", step1-start)\n",
    "    for i in range(nseg):\n",
    "        print(\"time to calculate segment\", time.time()-step1)\n",
    "        for channel_number, channel in enumerate(segs[i]):\n",
    "            # Convert the channel data to a Dask array\n",
    "            channel_da = da.from_array(channel, chunks=seg_len)\n",
    "            \n",
    "            # Apply taper and compute FFT using pyFFTW\n",
    "            fft_output = dafft.rfft(taper * channel_da).compute()\n",
    "            \n",
    "            # Compute the result\n",
    "            fourier_transformed = ((10 * np.log(np.abs(fft_output) ** 2)))[0:ind_f]\n",
    "            fourier_transformed[0] = 0\n",
    "            Fsegs[i][channel_number] = fourier_transformed\n",
    "    return Fsegs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "640f3330-922e-4392-b6e1-e0785635c485",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking method: pyfftw\n",
      "Time to create segments:  0.0002827644348144531\n",
      "time to calculate segment 9.608268737792969e-05\n",
      "time to calculate segment 7.6081647872924805\n",
      "time to calculate segment 15.087376594543457\n",
      "time to calculate segment 22.52371120452881\n",
      "time to calculate segment 30.099513053894043\n",
      "time to calculate segment 37.775598764419556\n",
      "time to calculate segment 45.30102229118347\n",
      "time to calculate segment 52.911967515945435\n",
      "time to calculate segment 60.48925495147705\n",
      "time to calculate segment 68.05755162239075\n",
      "time to calculate segment 75.67105388641357\n",
      "time to calculate segment 83.3500165939331\n",
      "time to calculate segment 91.00610542297363\n",
      "time to calculate segment 98.66999650001526\n",
      "time to calculate segment 106.36303210258484\n",
      "time to calculate segment 114.03694224357605\n",
      "time to calculate segment 121.70337414741516\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Benchmark the current method for the current file\u001b[39;00m\n\u001b[1;32m     38\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 39\u001b[0m Fsegs \u001b[38;5;241m=\u001b[39m \u001b[43mchannel_fourier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtaper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime elapsed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fft in file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end \u001b[38;5;241m-\u001b[39m start)\n",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m, in \u001b[0;36mchannel_fourier\u001b[0;34m(data, args, taper, positions, method)\u001b[0m\n\u001b[1;32m     21\u001b[0m channel_da \u001b[38;5;241m=\u001b[39m da\u001b[38;5;241m.\u001b[39mfrom_array(channel, chunks\u001b[38;5;241m=\u001b[39mseg_len)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Apply taper and compute FFT using pyFFTW\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m fft_output \u001b[38;5;241m=\u001b[39m dafft\u001b[38;5;241m.\u001b[39mrfft(\u001b[43mtaper\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchannel_da\u001b[49m)\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Compute the result\u001b[39;00m\n\u001b[1;32m     27\u001b[0m fourier_transformed \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(np\u001b[38;5;241m.\u001b[39mabs(fft_output) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)))[\u001b[38;5;241m0\u001b[39m:ind_f]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rhone-EOjzaUOU-py3.9/lib/python3.9/site-packages/dask/array/core.py:1587\u001b[0m, in \u001b[0;36mArray.__array_ufunc__\u001b[0;34m(self, numpy_ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m da_ufunc(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1586\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1587\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43melemwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_ufunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ufunc\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rhone-EOjzaUOU-py3.9/lib/python3.9/site-packages/dask/array/core.py:4818\u001b[0m, in \u001b[0;36melemwise\u001b[0;34m(op, out, where, dtype, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4813\u001b[0m     need_enforce_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   4814\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m is_scalar_for_elemwise(a) \u001b[38;5;129;01mand\u001b[39;00m a\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args\n\u001b[1;32m   4815\u001b[0m     )\n\u001b[1;32m   4817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n\u001b[0;32m-> 4818\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(op)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenize(op,\u001b[38;5;250m \u001b[39mdtype,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39margs,\u001b[38;5;250m \u001b[39mwhere)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4820\u001b[0m blockwise_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname, token\u001b[38;5;241m=\u001b[39mfuncname(op)\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   4822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rhone-EOjzaUOU-py3.9/lib/python3.9/site-packages/dask/base.py:1034\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(ensure_deterministic, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Deterministic token\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m>>> tokenize([1, 2, '3'])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m    Defaults to the `tokenize.ensure-deterministic` configuration parameter.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _seen_ctx(reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), _ensure_deterministic_ctx(ensure_deterministic):\n\u001b[0;32m-> 1034\u001b[0m     token: \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_normalize_seq_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m   1036\u001b[0m         token \u001b[38;5;241m=\u001b[39m token, _normalize_seq_func(\u001b[38;5;28msorted\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rhone-EOjzaUOU-py3.9/lib/python3.9/site-packages/dask/base.py:1160\u001b[0m, in \u001b[0;36m_normalize_seq_func\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1159\u001b[0m         seen[\u001b[38;5;28mid\u001b[39m(item)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(seen), item\n\u001b[0;32m-> 1160\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rhone-EOjzaUOU-py3.9/lib/python3.9/site-packages/dask/utils.py:773\u001b[0m, in \u001b[0;36mDispatch.__call__\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03mCall the corresponding method based on type of argument.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    772\u001b[0m meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\u001b[38;5;28mtype\u001b[39m(arg))\n\u001b[0;32m--> 773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rhone-EOjzaUOU-py3.9/lib/python3.9/site-packages/dask/base.py:1380\u001b[0m, in \u001b[0;36mregister_numpy.<locals>.normalize_ufunc\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;129m@normalize_token\u001b[39m\u001b[38;5;241m.\u001b[39mregister(np\u001b[38;5;241m.\u001b[39mufunc)\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_ufunc\u001b[39m(func):\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1380\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_normalize_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1382\u001b[0m         _maybe_raise_nondeterministic(\n\u001b[1;32m   1383\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot tokenize numpy ufunc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. Please use functions \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof the dask.array.ufunc module instead. See also \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.dask.org/en/latest/array-numpy-compatibility.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m         )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rhone-EOjzaUOU-py3.9/lib/python3.9/site-packages/dask/base.py:1241\u001b[0m, in \u001b[0;36m_normalize_pickle\u001b[0;34m(o)\u001b[0m\n\u001b[1;32m   1239\u001b[0m pik: \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     pik \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pik:\n\u001b[1;32m   1243\u001b[0m         pik \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming 'files' is a list of file paths and 'nFiles' is the total number of files\n",
    "nFiles = 2  # Set this to the actual number of files you want to process\n",
    "#methods = ['numpy', 'scipy', 'pyfftw']  # FFT methods to benchmark\n",
    "methods = ['pyfftw']\n",
    "#methods = ['scipy']\n",
    "# methods = ['numpy']\n",
    "\n",
    "# Loop over each method first\n",
    "for method in methods:\n",
    "    print(f\"Benchmarking method: {method}\")\n",
    "    # Then loop over each file for the current method\n",
    "    for file_index in range(nFiles):\n",
    "        seg_len = args[\"seg_len\"]\n",
    "        hop = args[\"hop\"]\n",
    "        N = args[\"N\"]\n",
    "\n",
    "        f = h5py.File(files[file_index], 'r')\n",
    "        dset = f['Acoustic']\n",
    "        data = np.array(dset)\n",
    "\n",
    "        taper = signal.windows.tukey(seg_len, 0.25)\n",
    "\n",
    "        if file_index != nFiles - 1:\n",
    "            g = h5py.File(files[file_index + 1], 'r')\n",
    "            dset2 = g['Acoustic']\n",
    "            data2 = np.array(dset2)\n",
    "            data = np.concatenate((data, data2[0:seg_len]), axis=0)\n",
    "\n",
    "        j = file_index + 1\n",
    "        file_pos = file_index * N\n",
    "\n",
    "        if file_index != nFiles - 1:\n",
    "            positions = np.arange(np.ceil((j - 1) * N / hop), np.floor((j * N - 1) / hop) + 1, dtype=int) * hop - file_pos\n",
    "        else:\n",
    "            positions = np.arange(np.ceil((j - 1) * N / hop), np.floor((j * N - seg_len) / hop) + 1, dtype=int) * hop - file_pos\n",
    "\n",
    "        # Benchmark the current method for the current file\n",
    "        start = time.time()\n",
    "        Fsegs = channel_fourier(data, args, taper, positions, method=method)\n",
    "        end = time.time()\n",
    "        print(f\"Time elapsed for {method} fft in file {file_index}:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8ad03-279f-4e78-bcbb-87dc7ee6d652",
   "metadata": {},
   "source": [
    "### Comparison of the improvements to the point\n",
    "Let's summarize our improvements so far and compare them together with the original setup:\n",
    "1. Reading with Xarray instead of  h5py\n",
    "2. Using pyFFTW with the builders module for the rfft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1372efb0-c599-49d6-a442-63d59573e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########Base settings#########\n",
    "#granularity of spectrogram\n",
    "freq_res = 1 # frequency resolution in Hz\n",
    "time_res = 0.1 # time res in seconds\n",
    "\n",
    "# section\n",
    "cable_start, cable_end = 0, 9200 # cable section to be processed (in meters) - 0==start\n",
    "start_channel_index, end_channel_index= cable_start // 4 , cable_end // 4 # channel distances (4m each)\n",
    "n_files = 5 # number of h5 files processed\n",
    "n_cores = 8 # cpu cores\n",
    "\n",
    "\n",
    "day= 22\n",
    "month=7\n",
    "sec=0\n",
    "minut=0\n",
    "hours=0\n",
    "\n",
    "# Additional parameters:\n",
    "file_length = 30 # Length of a single h5 file in seconds.\n",
    "sample_freq = 1000 #Sampling frequency in Hz of the recorded data.\n",
    "freq_max = 100 # maximum frequency cut off value for the analysis\n",
    "seg_length=1/freq_res #calculate window length corresponding to freq_res\n",
    "n_samples = file_length*sample_freq #number of samples in one file\n",
    "num_frequency_points = int(seg_length*freq_max+1)\n",
    "seg_len=int(seg_length*sample_freq) #how many time points should be in one processing window\n",
    "n_segments=int(2*(file_length/seg_length)) #amount of segments for the desired window length\n",
    "location_coords = np.arange(cable_start, cable_end, 4)\n",
    "freq_coords=scipy.fft.rfftfreq(int(sample_freq/freq_res), 1/sample_freq)[:num_frequency_points]\n",
    "hop = int(time_res*sample_freq)\n",
    "\n",
    "#fft input arguments\n",
    "args = {\n",
    "    \"num_frequency_points\" : num_frequency_points,\n",
    "    \"start_channel_index\" : start_channel_index,\n",
    "    \"end_channel_index\" : end_channel_index,\n",
    "    \"seg_len\" : seg_len,\n",
    "    \"hop\" : hop,\n",
    "    \"n_samples\" : n_samples\n",
    "}\n",
    "\n",
    "\n",
    "#path and name of resulting zarr-formatted data cube.\n",
    "ZARR_NAME = \"cryo_cube.zarr\"\n",
    "\n",
    "def channel_fourier(data, args, taper, positions, method='numpy'):\n",
    "    \"\"\"\n",
    "    Applies Fourier Transformation to segments of DAS records using specified method.\n",
    "\n",
    "    Args:\n",
    "        data (ndarray): The raw data from DAS channels.\n",
    "        args (dict): Contains parameters for Fourier Transform such as segment length and indices.\n",
    "        taper (ndarray): The taper function to apply before the Fourier transform.\n",
    "        positions (ndarray): The positions of the segments.\n",
    "        method (str): Method for FFT computation ('numpy', 'scipy', 'pyfftw'). Default is 'numpy'.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A 3D array containing the Fourier transform for each segment and channel.\n",
    "    \"\"\"\n",
    "    seg_len = args[\"seg_len\"]\n",
    "    end_channel_index, start_channel_index = args[\"end_channel_index\"], args[\"start_channel_index\"]\n",
    "    num_frequency_points = args[\"num_frequency_points\"]\n",
    "\n",
    "    segments = ([data[pos:pos+seg_len] for pos in positions])\n",
    "    segments = [seg.T[start_channel_index:end_channel_index] for seg in segments]\n",
    "\n",
    "    n_segments = positions.shape[0]\n",
    "    Fsegs = np.zeros((n_segments, end_channel_index-start_channel_index, num_frequency_points))\n",
    "\n",
    "    if method==\"pyfftw\":\n",
    "        # Pre-allocate the input array for FFTW\n",
    "        fft_input = pyfftw.empty_aligned(seg_len, dtype='float64')\n",
    "        # Create FFTW object\n",
    "        fft_object = pyfftw.builders.rfft(fft_input)#, planner_effort='FFTW_ESTIMATE') #, threads=mp.cpu_count()//2)\n",
    "\n",
    "        for i in range(n_segments):\n",
    "            for channel_number, channel in enumerate(segments[i]):\n",
    "                fft_input[:] = taper * channel  # Apply taper\n",
    "                fft_output = fft_object()  # Execute FFT\n",
    "                fourier_transformed = ((10 * np.log(np.abs(fft_output) ** 2)))[0:num_frequency_points] # Compute power spectrum\n",
    "                fourier_transformed[0] = 0 # Remove DC component (average value of the signal)\n",
    "                Fsegs[i][channel_number] = fourier_transformed\n",
    "    elif method==\"numpy\":\n",
    "        for i in range(n_segments):\n",
    "            for channel_number, channel in enumerate(segments[i]):\n",
    "\n",
    "                # note that modified_log(x)=10*log(x) (conversion to\n",
    "\n",
    "                fourier_transformed = np.fft.rfft(taper*channel, n=seg_len)\n",
    "                fourier_transformed = ((10*np.log(np.abs(fourier_transformed)**2)))[0:num_frequency_points]\n",
    "                fourier_transformed[0]=0\n",
    "                Fsegs[i][channel_number]=fourier_transformed\n",
    "    else:\n",
    "        print(\"method must be one of 'numpy' or 'pyfftw'\")\n",
    "        \n",
    "    return Fsegs\n",
    "\n",
    "\n",
    "\n",
    "def create_spectro_segment(file_index, args, filelist, method='h5py', fft_method='numpy'):\n",
    "    # chunk args\n",
    "    seg_len=args[\"seg_len\"]\n",
    "    hop=args[\"hop\"]\n",
    "    n_samples=args[\"n_samples\"]\n",
    "    filename=filelist[file_index]\n",
    "    \n",
    "    #taper function\n",
    "    taper = signal.windows.tukey(seg_len, 0.25)  # reduces the amplitude of the discontinuities at the boundaries, thereby reducing spectral leakage.\n",
    "    \n",
    "    data=[]\n",
    "    if method==\"xarray\":\n",
    "        xr_h5=xr.open_dataset(filename, engine='h5netcdf', backend_kwargs={'phony_dims': 'access'})\n",
    "        data=xr_h5[\"Acoustic\"].compute().values\n",
    "        \n",
    "        # the windowing function (Tukey window in this case) tapers at the ends, \n",
    "        # to avoid losing data at the ends of each file, \n",
    "        # the end of one file is overlapped with the beginning of the next file.\n",
    "        if file_index!=n_files-1:\n",
    "            xr_h5_2=xr.open_dataset(filelist[file_index+1], engine='h5netcdf', backend_kwargs={'phony_dims': 'access'})\n",
    "            data_2=xr_h5_2[\"Acoustic\"].compute().values\n",
    "            data = np.concatenate((data, data_2[0:seg_len]), axis=0)\n",
    "    elif method==\"h5py\":\n",
    "        h5file=h5py.File(filename,  'r')\n",
    "        dset=h5file['Acoustic']\n",
    "        data=np.array(dset) # DAS data\n",
    "        \n",
    "        if file_index!=n_files-1:\n",
    "            h5file_2 = h5py.File(files[file_index+1],'r')\n",
    "            dset_2=h5file_2['Acoustic']\n",
    "            data_2= np.array(dset_2)\n",
    "            data = np.concatenate((data, data_2[0:seg_len]), axis=0)\n",
    "    else:\n",
    "        print(\"method must be one of 'numpy' or 'pyfftw'\")\n",
    "    \n",
    "\n",
    "    next_file_index = file_index+1\n",
    "    file_pos = file_index * n_samples\n",
    "\n",
    "    # If the current file is not the last one\n",
    "    if file_index != n_files-1:\n",
    "        # Calculate the starting positions of each segment in the data\n",
    "        # first segment: (next_file_index-1)*n_samples/hop, rounded up\n",
    "        # last segment: (next_file_index*n_samples-1)/hop, rounded down\n",
    "        positions = np.arange(np.ceil((file_index)*n_samples/hop), np.floor((next_file_index*n_samples-1)/hop)+1, dtype=int)*hop - file_pos # scaled by the hop size and offset by the file position\n",
    "    else:\n",
    "        # If last one, start: (next_file_index*n_samples-seg_len)/hop\n",
    "        # to ensure that the last segment doesn't extend beyond the end of the data\n",
    "        positions = np.arange(np.ceil((file_index)*n_samples/hop), np.floor((next_file_index*n_samples-seg_len)/hop)+1, dtype=int)*hop - file_pos\n",
    "\n",
    "    Fsegs = channel_fourier(data, args, taper, positions, method=fft_method)\n",
    "    return Fsegs, positions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f44bdb0-1077-4f9c-9403-4053b86ce2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for h5py with numpy: 18.998287677764893\n",
      "Number of segments: 300\n",
      "Shape of the resulting array: (300, 2300, 101)\n",
      "Time elapsed for xarray with pyfftw: 16.11255121231079\n",
      "Number of segments: 300\n",
      "Shape of the resulting array: (300, 2300, 101)\n",
      "Total difference between the two arrays: 1.2833771878550948e-08\n",
      "Mean difference between the two arrays: 1.8415514246736905e-16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00, -2.84217094e-14,  0.00000000e+00, ...,\n",
       "          0.00000000e+00, -5.68434189e-14,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00, -2.84217094e-14],\n",
       "        ...,\n",
       "        [ 0.00000000e+00, -2.84217094e-14, -5.68434189e-14, ...,\n",
       "          1.42108547e-14,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  2.84217094e-14, ...,\n",
       "         -2.84217094e-14,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  2.84217094e-14]],\n",
       "\n",
       "       [[ 0.00000000e+00, -5.68434189e-14,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  5.68434189e-14],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 0.00000000e+00, -2.84217094e-14,  2.84217094e-14, ...,\n",
       "          4.26325641e-14,  4.26325641e-14,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  2.84217094e-14, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[  0.        , 249.65643724, 240.73256376, ..., 240.30308431,\n",
       "         243.85412415, 251.23577255],\n",
       "        [  0.        , 227.37218713, 236.30810147, ..., 236.49816194,\n",
       "         197.78252012, 247.87986118],\n",
       "        [  0.        , 235.13375845, 248.53002275, ..., 259.01518913,\n",
       "         250.36339076, 244.76471376],\n",
       "        ...,\n",
       "        [  0.        , 134.12702641, 123.63783194, ..., 111.90591666,\n",
       "         128.43574406, 135.38000592],\n",
       "        [  0.        , 144.41724364, 139.1639038 , ..., 150.49915646,\n",
       "         122.99808398, 155.72721874],\n",
       "        [  0.        , 143.32092329, 151.27369456, ..., 161.21458254,\n",
       "         150.07218085, 154.09740348]],\n",
       "\n",
       "       [[  0.        , 236.73555304, 239.86390695, ..., 239.25272624,\n",
       "         250.59239433, 242.20177736],\n",
       "        [  0.        , 216.32228499, 223.47404618, ..., 241.7678368 ,\n",
       "         224.12507733, 243.22383103],\n",
       "        [  0.        , 244.42657378, 239.07318284, ..., 260.66833352,\n",
       "         238.80873288, 242.20595459],\n",
       "        ...,\n",
       "        [  0.        , 131.06841013, 130.31413362, ..., 118.81623272,\n",
       "         111.76432764, 140.30656828],\n",
       "        [  0.        , 138.43512845, 143.09615478, ..., 148.22492055,\n",
       "         133.03602358, 151.61186869],\n",
       "        [  0.        , 137.13866416, 147.10583667, ..., 159.77853553,\n",
       "         154.20965831, 155.32196702]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start=time.time()\n",
    "# call the original function with h5py and numpy\n",
    "Fsegs_h5py, n_segments = create_spectro_segment(0, args, files, method='h5py', fft_method='numpy')\n",
    "print(\"Time elapsed for h5py with numpy:\", time.time()-start)\n",
    "print(\"Number of segments:\", n_segments)\n",
    "print(\"Shape of the resulting array:\", Fsegs_h5py.shape)\n",
    "\n",
    "start=time.time()\n",
    "# call the original function with xarray and pyfftw\n",
    "Fsegs_xarray, n_segments = create_spectro_segment(0, args, files, method='xarray', fft_method='pyfftw')\n",
    "print(\"Time elapsed for xarray with pyfftw:\", time.time()-start)\n",
    "print(\"Number of segments:\", n_segments)\n",
    "print(\"Shape of the resulting array:\", Fsegs_xarray.shape)\n",
    "\n",
    "# calculate the difference between the two arrays\n",
    "difference = Fsegs_h5py - Fsegs_xarray\n",
    "print(\"Total difference between the two arrays:\", np.sum(difference))\n",
    "print(\"Mean difference between the two arrays:\", np.mean(difference))\n",
    "display(difference[0:2])\n",
    "display(Fsegs_xarray[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c472ae-d73e-4843-a9af-0ffc3ccdbafe",
   "metadata": {},
   "source": [
    "### Profile the code to see further bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6b107dc-87c5-476a-aacd-5e9408e3d515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:\n",
      "Number of segments: 300\n",
      "Shape of the resulting array: (300, 2300, 101)\n",
      "Method:\n",
      "Number of segments: 300\n",
      "Shape of the resulting array: (300, 2300, 101)\n"
     ]
    }
   ],
   "source": [
    "def profile_function(method, fft_method):\n",
    "    # This function will be profiled\n",
    "    Fsegs, n_segments = create_spectro_segment(0, args, files, method=method, fft_method=fft_method)\n",
    "    print(\"Method:\".format(method, fft_method))\n",
    "    print(\"Number of segments:\", n_segments)\n",
    "    print(\"Shape of the resulting array:\", Fsegs.shape)\n",
    "\n",
    "\n",
    "def run_profiler():\n",
    "    profiler = cProfile.Profile()\n",
    "    \n",
    "    profiler.enable()\n",
    "    # Assuming profile_function is correctly defined and called\n",
    "    profile_function('h5py', 'numpy')\n",
    "    profile_function('xarray', 'pyfftw')\n",
    "    profiler.disable()\n",
    "\n",
    "    s = io.StringIO()\n",
    "    ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n",
    "    ps.print_stats()\n",
    "    s.flush()  # Ensure buffer is flushed\n",
    "    \n",
    "    return s.getvalue()\n",
    "\n",
    "stats = run_profiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f1c79d6-9957-4ce6-a703-f346b68f248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncalls</th>\n",
       "      <th>tottime</th>\n",
       "      <th>percall</th>\n",
       "      <th>cumtime</th>\n",
       "      <th>percall_again</th>\n",
       "      <th>filename:lineno(function)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>36.667</td>\n",
       "      <td>18.333</td>\n",
       "      <td>/tmp/ipykernel_1570349/845927988.py:1(profile_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.049</td>\n",
       "      <td>36.661</td>\n",
       "      <td>18.331</td>\n",
       "      <td>/tmp/ipykernel_1570349/1999376382.py:100(creat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>30.883</td>\n",
       "      <td>15.441</td>\n",
       "      <td>36.165</td>\n",
       "      <td>18.083</td>\n",
       "      <td>/tmp/ipykernel_1570349/1999376382.py:46(channe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>690000</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.204</td>\n",
       "      <td>0.000</td>\n",
       "      <td>/home/sc.uni-leipzig.de/ju554xqou/.cache/pypoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>690000</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.628</td>\n",
       "      <td>0.000</td>\n",
       "      <td>/home/sc.uni-leipzig.de/ju554xqou/.cache/pypoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>{method 'setdefault' of 'dict' objects}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>{method 'copy' of 'set' objects}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>/home/sc.uni-leipzig.de/ju554xqou/.cache/pypoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>/home/sc.uni-leipzig.de/ju554xqou/.cache/pypoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>/home/sc.uni-leipzig.de/ju554xqou/.cache/pypoe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>530 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ncalls tottime percall cumtime percall_again  \\\n",
       "1         2   0.004   0.002  36.667        18.333   \n",
       "2         2   0.099   0.049  36.661        18.331   \n",
       "3         2  30.883  15.441  36.165        18.083   \n",
       "4    690000   0.416   0.000   5.204         0.000   \n",
       "5    690000   0.479   0.000   4.628         0.000   \n",
       "..      ...     ...     ...     ...           ...   \n",
       "526       2   0.000   0.000   0.000         0.000   \n",
       "527       2   0.000   0.000   0.000         0.000   \n",
       "528       1   0.000   0.000   0.000         0.000   \n",
       "529       2   0.000   0.000   0.000         0.000   \n",
       "530       1   0.000   0.000   0.000         0.000   \n",
       "\n",
       "                             filename:lineno(function)  \n",
       "1    /tmp/ipykernel_1570349/845927988.py:1(profile_...  \n",
       "2    /tmp/ipykernel_1570349/1999376382.py:100(creat...  \n",
       "3    /tmp/ipykernel_1570349/1999376382.py:46(channe...  \n",
       "4    /home/sc.uni-leipzig.de/ju554xqou/.cache/pypoe...  \n",
       "5    /home/sc.uni-leipzig.de/ju554xqou/.cache/pypoe...  \n",
       "..                                                 ...  \n",
       "526            {method 'setdefault' of 'dict' objects}  \n",
       "527                   {method 'copy' of 'set' objects}  \n",
       "528  /home/sc.uni-leipzig.de/ju554xqou/.cache/pypoe...  \n",
       "529  /home/sc.uni-leipzig.de/ju554xqou/.cache/pypoe...  \n",
       "530  /home/sc.uni-leipzig.de/ju554xqou/.cache/pypoe...  \n",
       "\n",
       "[530 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adjusted parsing logic\n",
    "output_lines = stats.split('\\n')\n",
    "rows = []\n",
    "for line in output_lines:\n",
    "    # Use a more sophisticated method to split lines into columns, considering potential spaces in the last column\n",
    "    cols = line.split(maxsplit=5)  # This assumes the first 5 columns do not contain spaces that should be preserved\n",
    "    if len(cols) > 5 and cols[0].replace('.', '', 1).isdigit():\n",
    "        rows.append(cols)\n",
    "\n",
    "# Update column names based on actual data structure and inspection of the rows\n",
    "df_columns = ['ncalls', 'tottime', 'percall', 'cumtime', 'percall_again', 'filename:lineno(function)']\n",
    "# Ensure the number of columns in df_columns matches the actual data\n",
    "df = pd.DataFrame(rows, columns=df_columns)\n",
    "df = df.iloc[1:]\n",
    "pd.set_option('display.max_rows', 200)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data-praktikum",
   "language": "python",
   "name": "rhone-eojzauou-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
