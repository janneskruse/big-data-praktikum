{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase the pipeline and Cube visualization\n",
    "\n",
    "In [slurm/02_fft_pipeline.py](../slurm/02_fft_pipeline.py) there is a pipeline to execute the cube creation for all files and folders in the respective repository. Here we showcase this pipeline with the Fast Fourier Transformation and the Wavelet Transformation included for a small number of files and without storing the resulting cube.\n",
    "\n",
    "The second part of the notebook visualizes the resulting cube using the [lexcube](https://github.com/msoechting/lexcube) package.\n",
    "\n",
    "You can test if lexcube is working by running the following random data cube:\n",
    "\n",
    "```python\n",
    "#########testing the lexcube installation#####\n",
    "data_source = np.sum(np.mgrid[0:256,0:256,0:256], axis=0)\n",
    "w = lc.Cube3DWidget(data_source, cmap=\"prism\", vmin=0, vmax=768)\n",
    "w\n",
    "```\n",
    "\n",
    "If you struggle setting it up, please refer to the instructions in the [condaenv folder's](../condaenv/) README.md."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline\n",
    "1. Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Import the necessary modules #############\n",
    "#python basemodules and jupyter modules\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import psutil\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from operator import itemgetter\n",
    "import fnmatch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# get the base path of the repository\n",
    "repo_dir = os.popen('git rev-parse --show-toplevel').read().strip()\n",
    "###load the .env file\n",
    "load_dotenv(dotenv_path=f\"{repo_dir}/.env\")\n",
    "print(f\"FFTW library loaded: {os.environ.get('LD_LIBRARY_PATH')}\")\n",
    "\n",
    "#benchmarking\n",
    "import time\n",
    "# import cProfile\n",
    "# import pstats\n",
    "# import io\n",
    "\n",
    "# data handling\n",
    "import h5py\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import signal #, fft\n",
    "import pyfftw\n",
    "#import pyfftw.interfaces.dask_fft as dafft\n",
    "# import pickle\n",
    "# import zarr\n",
    "\n",
    "#visualization\n",
    "import lexcube as lc\n",
    "from itables import init_notebook_mode, show\n",
    "init_notebook_mode(all_interactive=False)\n",
    "\n",
    "############# Parse the command line arguments #############\n",
    "total_cpus = int(mp.cpu_count())\n",
    "\n",
    "###########get the folder environment vairables#########\n",
    "base=os.getenv(\"BASE_FOLDER\")\n",
    "zarr_base=os.getenv(\"ZARR_BASE_FOLDER\")\n",
    "print(f\"Base data folder:{base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run the pipeline for a small amount of files (default 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_process=10\n",
    "startTime = time.time()\n",
    "\n",
    "\n",
    "############# Define the functions #############\n",
    "def get_sorted_folders (base):\n",
    "    \"\"\"\n",
    "    Groups folders by date and sorts them chronologically.\n",
    "    \n",
    "    Args:\n",
    "        base (str): The base folder to search for days.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dates in the format \"YYYYMMDD\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Change to the base directory\n",
    "    os.chdir(base)\n",
    "    folders = os.listdir()\n",
    "    \n",
    "    # Define the date pattern\n",
    "    date_pattern = re.compile(r\"(\\d{8})_?\\d*\")  # Match the date in the folder name\n",
    "    date_folders = {}\n",
    "\n",
    "    # Group folders by date\n",
    "    for folder in folders:\n",
    "        match = date_pattern.match(folder)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            if date_str in date_folders:\n",
    "                date_folders[date_str].append(folder)\n",
    "            else:\n",
    "                date_folders[date_str] = [folder]\n",
    "\n",
    "    print(\"Number of folders before moving files:\", len(folders))\n",
    "    \n",
    "    # Sort folders within each date group\n",
    "    for date in date_folders:\n",
    "        date_folders[date].sort(key=lambda x: (x.split('_')[0], int(x.split('_')[1]) if '_' in x else 0))\n",
    "    \n",
    "    # Combine folders for each date where there are multiple folders\n",
    "    for date, folders in date_folders.items():\n",
    "        if len(folders) > 1:\n",
    "            combine_folders_with_same_date(folders)\n",
    "    \n",
    "    os.chdir(base)\n",
    "    print(\"Number of folders after moving files:\", len(os.listdir()))        \n",
    "            \n",
    "    # sort the dates\n",
    "    date_folders = dict(sorted(date_folders.items(), key=lambda x: x[0]))        \n",
    "    \n",
    "    return list(date_folders.keys())\n",
    "\n",
    "def combine_folders_with_same_date(folders):\n",
    "    \"\"\"\n",
    "    Combines folders with the same date into one folder.\n",
    "\n",
    "    Args:\n",
    "        base (str): The base folder to search for days.\n",
    "    \"\"\"\n",
    "    primary_folder = folders[0]\n",
    "    folder_path = os.path.join(base, folders[1])\n",
    "    primary_folder_path = os.path.join(base, primary_folder)\n",
    "    \n",
    "    # Move contents to the primary folder\n",
    "    files=os.listdir(folder_path)\n",
    "        \n",
    "    pool=mp.Pool(mp.cpu_count())\n",
    "    pool.starmap(move_files, [(filename, folder_path, primary_folder_path) for filename in files])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Remove the now-empty folder\n",
    "    os.system(f\"rm -rf {folder_path}\")\n",
    "    print(f\"Combined {folders[1]} into {primary_folder}.\") \n",
    "\n",
    "def move_files(filename, folder_path, primary_folder_path):\n",
    "    \"\"\"\n",
    "    Moves a file from a folder to the primary folder.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The name of the file to move.\n",
    "        folder_path (str): The path to the folder containing the file.\n",
    "        primary_folder_path (str): The path to the primary folder.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.join(primary_folder_path, filename)): # Check if the file already exists in the primary folder\n",
    "        join=os.path.join(folder_path, filename)\n",
    "        shutil.move(join, primary_folder_path) # Move the file to the primary folder\n",
    "    # else:\n",
    "    #     print(\"already exists\")\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    \"\"\"\n",
    "    Extracts the timestamp from a DAS-h5-file's filename.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The filename to extract the timestamp from.\n",
    "        \n",
    "    Returns:\n",
    "        str: The timestamp part of the filename.\n",
    "    \"\"\"\n",
    "    # for the format is 'rhone1khz_UTC_yyyymmdd_hhmmss.ms.h5'\n",
    "    timestamp_part = filename.split('_')[2] + filename.split('_')[3]\n",
    "    return timestamp_part\n",
    "\n",
    "def get_filenames(folder, base):\n",
    "    \"\"\"\n",
    "    Collects the filenames in the data folder and sorts them by time.\n",
    "\n",
    "    Args:\n",
    "        folder (str): The folder to search for files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are integers and values are filenames, sorted chronologically.\n",
    "    \"\"\"\n",
    "    # Change to the folder directory\n",
    "    folder_path = os.path.join(base, folder)\n",
    "    os.chdir(folder_path)\n",
    "    files=os.listdir()\n",
    "    \n",
    "    # filter files to only get those where the filename contains 'rhone2das' or 'rhone1khz'\n",
    "    # Use fnmatch.filter to find matches for both patterns in a single pass\n",
    "    pattern_matches = fnmatch.filter(files, '*rhone2das*') + fnmatch.filter(files, '*rhone1khz*')\n",
    "    \n",
    "    # Sort the files by timestamp\n",
    "    sorted_files = sorted(pattern_matches, key=extract_timestamp)\n",
    "    \n",
    "    return  sorted_files \n",
    "\n",
    "def channel_wavelet_fcwt(data, seg_len, hop, NU, freq_max, ind_a, ind_e):\n",
    "    \"\"\"\n",
    "    Applies Wavelet Transformation to segments of DAS records to compute time-frequency representations.\n",
    "\n",
    "    Args:\n",
    "        data (ndarray): The raw data from DAS channels.\n",
    "        seg_len (int): Segment length.\n",
    "        hop (int): Hop size.\n",
    "        NU (int): Sampling frequency.\n",
    "        freq_max (int): Maximum frequency for wavelet transformation.\n",
    "        ind_a (int): Start index for channels.\n",
    "        ind_e (int): End index for channels.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A 3D array containing the Wavelet transform for each segment and channel.\n",
    "    \"\"\"\n",
    "    \n",
    "    seg_len = args[\"seg_len\"]\n",
    "    hop = args[\"hop\"]\n",
    "    freq_max = args[\"freq_max\"]\n",
    "    ind_a = args[\"start_channel_index\"]\n",
    "    ind_e = args[\"end_channel_index\"]\n",
    "    NU = args[\"sample_freq\"]\n",
    "    \n",
    "    \n",
    "    positions = np.arange(0, data.shape[0] - seg_len, hop)\n",
    "    segs = [data[pos:pos + seg_len] for pos in positions]  # Dividing the data into segments\n",
    "    segs = [seg.T[ind_a:ind_e] for seg in segs]  # Transposing the segments to get time series for each channel\n",
    "    nseg = len(segs)\n",
    "    nfreqs = freq_max  # Number of frequency bins\n",
    "\n",
    "    Wsegs = np.zeros((nseg, ind_e - ind_a, nfreqs))\n",
    "\n",
    "    # Inside the channel_wavelet_fcwt function, adjust the nfreqs based on actual cwt_data output\n",
    "    for i in range(nseg):\n",
    "        for channel_number, channel in enumerate(segs[i]):\n",
    "            freqs, cwt_data = fcwt.cwt(channel, NU, 1, freq_max, nfreqs)\n",
    "            # Update nfreqs based on the actual output size of cwt_data, if necessary\n",
    "            nfreqs_actual = cwt_data.shape[1]  # Assuming cwt_data shape is (time_points, frequency_bins)\n",
    "            if i == 0 and channel_number == 0:  # Adjust the Wsegs array size only once\n",
    "                Wsegs = np.zeros((nseg, ind_e - ind_a, nfreqs_actual))\n",
    "            mean_cwt_data = np.mean(10 * np.log(np.abs(cwt_data)**2), axis=0)\n",
    "            Wsegs[i][channel_number] = mean_cwt_data\n",
    "    \n",
    "    return Wsegs\n",
    "\n",
    "def channel_fourier(data, args, taper, positions):\n",
    "    \"\"\"\n",
    "    Applies the Fourier transform to segments of the DAS records using the pyFFTW library.\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): The DAS data.\n",
    "        args (dict): The arguments for the Fourier transform.\n",
    "            Args requires the following keys:\n",
    "                \"fft_dtype\" (str): The data type for the Fourier transform.\n",
    "                \"num_frequency_points\" (int): The number of frequency points.\n",
    "                \"start_channel_index\" (int): The start channel index.\n",
    "                \"end_channel_index\" (int): The end channel index.\n",
    "                \"seg_len\" (int): The segment length.\n",
    "                \"hop\" (int): The hop size.\n",
    "                \"n_samples\" (int): The number of samples.\n",
    "        taper (np.array): The taper function.\n",
    "        positions (np.array): The positions of the segments in the data.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: The Fourier transformed segments.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack the arguments\n",
    "    seg_len = args[\"seg_len\"]\n",
    "    end_channel_index, start_channel_index = args[\"end_channel_index\"], args[\"start_channel_index\"]\n",
    "    fft_dtype = args[\"fft_dtype\"]  # dtype: float32\n",
    "    n_segments = positions.shape[0]\n",
    "\n",
    "    # Pre-allocate the segments array\n",
    "    segments = ([data[pos:pos+seg_len] for pos in positions])\n",
    "    segments = [seg.T[start_channel_index:end_channel_index] for seg in segments]\n",
    "    Fsegs = np.zeros((n_segments, end_channel_index - start_channel_index, num_frequency_points), dtype=fft_dtype)  # empty float32 array\n",
    "    \n",
    "    # Pre-allocate the input array for FFTW\n",
    "    fft_input = pyfftw.empty_aligned(seg_len, dtype=fft_dtype)\n",
    "    # Create the FFTW object\n",
    "    fft_object = pyfftw.builders.rfft(fft_input)  # , planner_effort='FFTW_ESTIMATE') #, threads=mp.cpu_count()//2)\n",
    "    \n",
    "    for i in range(n_segments):\n",
    "        for channel_number, channel in enumerate(segments[i]):\n",
    "            fft_input[:] = taper * channel  # Apply taper\n",
    "            #np.multiply(taper, channel, out=fft_input)  # Apply taper\n",
    "            fft_output = fft_object()  # Execute FFT\n",
    "            fourier_transformed = (10 * np.log(np.abs(fft_output) ** 2 + 1e-10))[:num_frequency_points]  # Compute power spectrum\n",
    "            fourier_transformed[0] = 0  # Remove DC component (average value of the signal)\n",
    "            Fsegs[i][channel_number] = fourier_transformed\n",
    "    \n",
    "    return Fsegs  # return the Fourier transformed segments\n",
    "\n",
    " \n",
    "\n",
    "def create_spectro_segment(file_index, args, filelist):\n",
    "    \"\"\"\n",
    "    Creates a spectrogram segment from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_index (int): The index of the file.\n",
    "        args (dict): The arguments for the Fourier transform.\n",
    "            Args requires the following keys:\n",
    "                \"n_files\" (int): The number of files.\n",
    "                \"seg_len\" (int): The segment length.\n",
    "                \"hop\" (int): The hop size.\n",
    "                \"n_samples\" (int): The number of samples.\n",
    "        filelist (list): The list of file names.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: The Fourier transformed segments.\n",
    "        int: The number of segments.\n",
    "    \"\"\"\n",
    "    \n",
    "    # chunk args\n",
    "    n_files=args[\"n_files\"]\n",
    "    seg_len=args[\"seg_len\"]\n",
    "    hop=args[\"hop\"]\n",
    "    n_samples=args[\"n_samples\"]\n",
    "    filename=filelist[file_index]\n",
    "    float_type=args[\"fft_dtype\"]\n",
    "    \n",
    "    #taper function\n",
    "    taper = signal.windows.tukey(seg_len, 0.25)  # reduces the amplitude of the discontinuities at the boundaries, thereby reducing spectral leakage.\n",
    "    \n",
    "    # Load the data\n",
    "    # xr_h5 = xr.open_dataset(filename, engine='h5netcdf', backend_kwargs={'phony_dims': 'access'})\n",
    "    # data = xr_h5[\"Acoustic\"].compute().values.astype(float_type)\n",
    "    \n",
    "    f = h5py.File(filename,'r')\n",
    "    dset=f['Acoustic']\n",
    "    data = np.array(dset, dtype=float_type)\n",
    "    \n",
    "    if file_index < n_files - 1:\n",
    "        #xr_h5_2 = xr.open_dataset(filelist[file_index + 1], engine='h5netcdf', backend_kwargs={'phony_dims': 'access'})\n",
    "        #data_2 = xr_h5_2[\"Acoustic\"].compute().values.astype(float_type)\n",
    "        f = h5py.File(filelist[file_index+1],'r')\n",
    "        dset=f['Acoustic']\n",
    "        data_2 = np.array(dset, dtype=float_type)\n",
    "        data = np.concatenate((data, data_2[0:seg_len]), axis=0)\n",
    "    \n",
    "    next_file_index = file_index+1\n",
    "    file_pos = file_index * n_samples\n",
    "\n",
    "    # If the current file is not the last one\n",
    "    if file_index != n_files-1:\n",
    "        # Calculate the starting positions of each segment in the data\n",
    "        # first segment: (next_file_index-1)*n_samples/hop, rounded up\n",
    "        # last segment: (next_file_index*n_samples-1)/hop, rounded down\n",
    "        positions = np.arange(np.ceil((file_index)*n_samples/hop), np.floor((next_file_index*n_samples-1)/hop)+1, dtype=int)*hop - file_pos # scaled by the hop size and offset by the file position\n",
    "    else:\n",
    "        # If last one, start: (next_file_index*n_samples-seg_len)/hop\n",
    "        # to ensure that the last segment doesn't extend beyond the end of the data\n",
    "        positions = np.arange(np.ceil((file_index)*n_samples/hop), np.floor((next_file_index*n_samples-seg_len)/hop)+1, dtype=int)*hop - file_pos\n",
    "\n",
    "    \n",
    "    # Filter positions to ensure valid segments\n",
    "    positions = positions[positions + seg_len <= len(data)]\n",
    "    \n",
    "    start=time.time()\n",
    "    Fsegs = channel_fourier(data, args, taper, positions)\n",
    "    print(f\"Time taken for fft of {filename}: {time.time()-start}\")\n",
    "    \n",
    "    start=time.time()\n",
    "    Wsegs = channel_wavelet_fcwt(data, args)\n",
    "    print(f\"Time taken for wavelet transformation of {filename}: {time.time()-start}\")\n",
    "    \n",
    "    return Fsegs, Wsegs, positions.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########Base settings#########\n",
    "#granularity of spectrogram\n",
    "freq_res = int(os.getenv(\"FREQ_RES\", 1)) # frequency resolution in Hz\n",
    "time_res = float(os.getenv(\"TIME_RES\", 0.1)) # time res in seconds\n",
    "float_type = os.getenv(\"FLOAT_TYPE\", 'float32')\n",
    "\n",
    "# section\n",
    "channel_distance = int(os.getenv(\"CHANNEL_DISTANCE\", 4)) # distance between channels in meters\n",
    "cable_start = int(os.getenv(\"CABLE_START\", 0)) # cable section to be processed (in meters) - 0==start\n",
    "cable_end = int(os.getenv(\"CABLE_END\", 9200)) # cable section to be processed (in meters) - 0==start\n",
    "start_channel_index, end_channel_index = cable_start // channel_distance, cable_end // channel_distance # channel distances to indices\n",
    "expected_channels = end_channel_index - start_channel_index # expected number of channels\n",
    "\n",
    "# Additional parameters:\n",
    "file_length = int(os.getenv(\"FILE_LENGTH\", 30)) # Length of a single h5 file in seconds.\n",
    "sample_freq = int(os.getenv(\"SAMPLE_FREQ\", 1000)) # Sampling frequency in Hz of the recorded data.\n",
    "freq_max = int(os.getenv(\"FREQ_MAX\", 100)) # maximum frequency cut off value for the analysis\n",
    "seg_length=1/freq_res #calculate window length corresponding to freq_res\n",
    "n_samples = file_length*sample_freq #number of samples in one file/total number of data points available in one file\n",
    "num_frequency_points = int(seg_length*freq_max+1)\n",
    "seg_sample_len=int(seg_length*sample_freq) # how many time points should be in one processing window\n",
    "n_segments_file=int(2*(file_length/seg_length)) # amount of segments for the desired window length\n",
    "location_coords = np.arange(cable_start, cable_end, 4) # channel locations\n",
    "freq_coords=scipy.fft.rfftfreq(int(sample_freq/freq_res), 1/sample_freq)[:num_frequency_points] # frequency coordinates\n",
    "hop = int(time_res*sample_freq) # hop size - how many samples to skip between segments\n",
    "\n",
    "#fft input arguments\n",
    "args = {\n",
    "    \"fft_dtype\": float_type,\n",
    "    \"num_frequency_points\" : num_frequency_points,\n",
    "    \"start_channel_index\" : start_channel_index,\n",
    "    \"end_channel_index\" : end_channel_index,\n",
    "    \"seg_len\" : seg_sample_len,\n",
    "    \"hop\" : hop,\n",
    "    \"n_samples\" : n_samples,\n",
    "    \"seg_length\" : seg_length,\n",
    "    \"freq_max\" : freq_max,\n",
    "    \"sample_freq\" : sample_freq,\n",
    "    \"expected_channels\" : expected_channels\n",
    "}\n",
    "\n",
    "\n",
    "##########Main#########\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    # set the folder\n",
    "    folders=get_sorted_folders(base)\n",
    "    folder=folders[0]\n",
    "    \n",
    "    #path and name of resulting zarr-formatted data cube.\n",
    "    zarr_name = f\"cryo_cube_{folder}.zarr\"\n",
    "    zarr_path = f\"{zarr_base}/{zarr_name}\"\n",
    "    \n",
    "    os.chdir(base) # change to the base directory\n",
    "    print(f\"base folder: {os.getcwd()}\")\n",
    "    while os.path.exists(zarr_path) and folders:  # Check if folders is not empty\n",
    "        folder = folders.pop(0)  # remove and return the first element\n",
    "        zarr_name = f\"cryo_cube_{folder}.zarr\"\n",
    "        zarr_path = f\"{zarr_base}/{zarr_name}\"\n",
    "    \n",
    "    if not folders:\n",
    "        print(\"No more folders to process.\")\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        print(f\"Processing folder {folder}\")\n",
    "    \n",
    "    #get the day and month\n",
    "    day=folder[6:8]\n",
    "    month=folder[4:6]\n",
    "    \n",
    "    # print the settings\n",
    "    print(20*\"*\")\n",
    "    print(\"Max number of CPUs: \", total_cpus)\n",
    "    print(f\"Processed day: {day}.{month}.2020\")\n",
    "    print(f\"Time resolution: {time_res} sec\")\n",
    "    print(f\"Frequency resolution: {freq_res} Hz\")\n",
    "    print(f\"Resulting overlap: {1-hop/seg_sample_len}\")\n",
    "    print(f\"Expected number of channels: {expected_channels}\")\n",
    "    print(10*\"*\")\n",
    "    \n",
    "    # get the filenames and the total amount of segments\n",
    "    filenames = get_filenames(folder, base)[0:int(files_to_process)]\n",
    "    n_files=len(filenames)\n",
    "    args[\"n_files\"] = n_files\n",
    "    print(\"Number of files:\", n_files)\n",
    "    print(\"filenames\", filenames)\n",
    "    n_segments_total = int(np.floor((n_files*n_samples-seg_sample_len)/hop))+1 # total amount of segments\n",
    "\n",
    "    print(f\"Creating zarr shape...\")\n",
    "    # creating zarr shape\n",
    "    z_shape=(n_segments_total, expected_channels, num_frequency_points) \n",
    "    z_chunks=(n_segments_file,expected_channels,num_frequency_points)\n",
    "\n",
    "    print(\"Creating metadata...\")\n",
    "    start=time.time()\n",
    "\n",
    "    # Generate time coordinates based on the first file\n",
    "    dummy_file_path=os.path.join(base, folder, filenames[0])\n",
    "    dummy_xr = xr.open_dataset(filenames[0], engine='h5netcdf', backend_kwargs={'phony_dims': 'access'})\n",
    "    attr = dummy_xr['Acoustic'].attrs\n",
    "    start_time = np.datetime64(attr[\"ISO8601 Timestamp\"], 'ns') # Get the start time of the first file\n",
    "    time_res_ms = time_res * 1000  # Convert time_res from seconds to milliseconds\n",
    "    time_coords = start_time + np.arange(n_segments_total) * np.timedelta64(int(time_res_ms), 'ms') \n",
    "    \n",
    "    \n",
    "    fft_dask = da.zeros(z_shape, chunks=z_chunks, dtype=float_type) # create an empty dask array\n",
    "\n",
    "    xr_zarr = xr.Dataset(\n",
    "        {\n",
    "            \"fft\": ([\"time\", \"channel\", \"frequency\"], fft_dask),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": time_coords,\n",
    "            \"channel\": location_coords,\n",
    "            \"frequency\": freq_coords,\n",
    "        },\n",
    "    )\n",
    "    print(xr_zarr)\n",
    "        \n",
    "    print(f\"metadata created in {time.time()-start}s:\")\n",
    "\n",
    "    #xarray dataset to zarr\n",
    "    print(f\"Creating and writing empty {zarr_path} with metadata...\")\n",
    "    start=time.time()\n",
    "    #xr_zarr.to_zarr(zarr_path, mode='w', consolidated=True)\n",
    "    print(f\"zarr created in {time.time()-start}s\")\n",
    "     \n",
    "    # In the following lines, multiple cpu-cores calculate\n",
    "    # a fft for each file simultanously.\n",
    "    # Before that we split the whole data to be processed in to not overload the memory!\n",
    "\n",
    "    # Determine available system memory \n",
    "    available_memory = psutil.virtual_memory().available * 0.8  # Use 80% of available memory (let's reserve some memory for the system and other processes)\n",
    "\n",
    "    # Calculate how many files can be processed simultaneously\n",
    "    memory_per_file = os.path.getsize(dummy_file_path)*1.5 # 1.5 times the size of the file, assuming some overhead\n",
    "    print(\"Memory per file (MB):\", memory_per_file / (1024**2))\n",
    "    files_at_once = int(available_memory / memory_per_file)\n",
    "    files_at_once = max(1, files_at_once) # Ensure that at least one file is processed at once\n",
    "\n",
    "    # Calculate the number of divisions \n",
    "    n_div = max(1, n_files // files_at_once)\n",
    "    index_list = np.arange(n_files)\n",
    "\n",
    "    # set split up\n",
    "    if n_files > files_at_once:\n",
    "        split_up = np.array_split(index_list, n_div)\n",
    "    else:\n",
    "        split_up = [index_list]\n",
    "\n",
    "    # Define the number of cores to be 90% of available cores, rounded down\n",
    "    n_cores = int(total_cpus * 0.9) // 1\n",
    "    print(\"Number of cores used:\", n_cores)\n",
    "\n",
    "    startT = time.time() # start the timer\n",
    "\n",
    "    running_index=0\n",
    "    for liste in split_up:\n",
    "        \n",
    "        print(f\"Starting FFT for split_up liste {liste}\")\n",
    "        # Start the local timer\n",
    "        start = time.time() \n",
    "        \n",
    "        # multiprocessing the fft calculation\n",
    "        pool=mp.Pool(n_cores)\n",
    "        fft_results=pool.starmap(create_spectro_segment, [(i, args, filenames) for i in liste])\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        # Print the time taken to process the files\n",
    "        end=time.time() # end the local timer\n",
    "        fft_time=end-start\n",
    "        time_per_file=fft_time/len(liste)\n",
    "        \n",
    "        print(20*\"*\")\n",
    "        print(f\"Time taken for fft from splitup: {fft_time}\")\n",
    "        print(f\"Time per File: {time_per_file}\")\n",
    "        print(20*\"*\")\n",
    "        \n",
    "        fft_results = list(fft_results) # convert the map object to a list\n",
    "        \n",
    "        print(\"Writing liste to zarr...\")\n",
    "        start=time.time()\n",
    "        \n",
    "        # Convert results to Dask arrays and store them\n",
    "        for i in liste:\n",
    "            Fsegs, Wsegs, nseg = fft_results[i-int(liste[0])]\n",
    "            nseg = int(nseg)\n",
    "            fft_dask_array = da.from_array(Fsegs, chunks=(nseg, expected_channels, num_frequency_points))\n",
    "            cwt_dask_array = da.from_array(Wsegs, chunks=(nseg, expected_channels, num_frequency_points))\n",
    "            xr_zarr[\"fft\"][running_index:running_index+nseg] = fft_dask_array\n",
    "            xr_zarr[\"cwt\"][running_index:running_index+nseg] = cwt_dask_array\n",
    "            running_index += nseg\n",
    "            \n",
    "        #xr_zarr.to_zarr(zarr_path, mode='a', consolidated=True)\n",
    "        print(f\"Wrote FFT to zarr using Dask in {time.time()-start}s for split_up {liste}\")\n",
    "        \n",
    "    print(20*\"*\")\n",
    "    print(\"Calculation completed\")\n",
    "    print(\"Total computation time in seconds:\", time.time()-startTime) \n",
    "    print(\"Computation time in seconds for fft and Zarr storage:\", time.time()-startT)\n",
    "    print(\"Number of processed files:\", n_files)\n",
    "    print(\"Number of used cores:\", n_cores)\n",
    "    print(20*\"*\")\n",
    "    \n",
    "    # submit the script again\n",
    "    if len(folders)>30: #\n",
    "        print(f\"Submitting the script again to process the next folder {folders[0]}\")\n",
    "        os.chdir(f\"{repo_dir}/code/slurm\")\n",
    "        #os.system(f\"sbatch 02_fft_pipeline.sh\") #uncomment this to autmatically submit the next script for the next folder when finished\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Showing the newly created xarray cube and its object structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Visualize the fft attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fft = lc.Cube3DWidget(xr_zarr.fft.T)\n",
    "w_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualize the wavelet attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_cwt = lc.Cube3DWidget(xr_zarr.fft.T)\n",
    "w_cwt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sliders to interact with the cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_cwt.show_sliders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already created a cube via the slurm pipeline, you can open it easily here. Commented out some simple ways on how to interact with an xarray object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(zarr_base)\n",
    "zarrs=os.listdir()\n",
    "testcube=xr.open_dataset(zarrs[0], chunks={}, engine=\"zarr\")\n",
    "#test.fft.sel(time='2020-07-04T15:50:30.049000000').plot()\n",
    "#test=testcube.fft[0:10000].T #[0:10,0:10,0:10]\n",
    "test=testcube.fft[0:100].compute().values #compute loads the data into memory (in a distributed manner)\n",
    "testcube"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
