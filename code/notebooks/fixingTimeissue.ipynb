{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ac0af6-ef94-4e0c-b147-0c3e888d85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Import the necessary modules #############\n",
    "#python basemodules and jupyter modules\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import psutil\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from operator import itemgetter\n",
    "import fnmatch\n",
    "\n",
    "#benchmarking\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "# data handling\n",
    "import h5py\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import signal, fft\n",
    "import pyfftw\n",
    "import pyfftw.interfaces.dask_fft as dafft\n",
    "import pickle\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230258aa-bd64-46a7-a191-e78c44384e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Parse the command line arguments #############\n",
    "total_cpus = int(sys.argv[1])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "############# Base paths and folder names #############\n",
    "# get the base path of the repository\n",
    "repo_dir = os.popen('git rev-parse --show-toplevel').read().strip()\n",
    "base=\"/work/le837wmue-Rhone_download/DAS_2020\"\n",
    "zarr_base=\"/work/ju554xqou-rhonezarrs\"\n",
    "\n",
    "\n",
    "############# Define the functions #############\n",
    "def get_sorted_folders (base):\n",
    "    \"\"\"\n",
    "    Groups folders by date and sorts them chronologically.\n",
    "    \n",
    "    Args:\n",
    "        base (str): The base folder to search for days.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dates in the format \"YYYYMMDD\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Change to the base directory\n",
    "    os.chdir(base)\n",
    "    folders = os.listdir()\n",
    "    \n",
    "    # Define the date pattern\n",
    "    date_pattern = re.compile(r\"(\\d{8})_?\\d*\")  # Match the date in the folder name\n",
    "    date_folders = {}\n",
    "\n",
    "    # Group folders by date\n",
    "    for folder in folders:\n",
    "        match = date_pattern.match(folder)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            if date_str in date_folders:\n",
    "                date_folders[date_str].append(folder)\n",
    "            else:\n",
    "                date_folders[date_str] = [folder]\n",
    "\n",
    "    print(\"Number of folders before moving files:\", len(folders))\n",
    "    \n",
    "    # Sort folders within each date group\n",
    "    for date in date_folders:\n",
    "        date_folders[date].sort(key=lambda x: (x.split('_')[0], int(x.split('_')[1]) if '_' in x else 0))\n",
    "    \n",
    "    # Combine folders for each date where there are multiple folders\n",
    "    for date, folders in date_folders.items():\n",
    "        if len(folders) > 1:\n",
    "            combine_folders_with_same_date(folders)\n",
    "    \n",
    "    os.chdir(base)\n",
    "    print(\"Number of folders after moving files:\", len(os.listdir()))        \n",
    "            \n",
    "    # sort the dates\n",
    "    date_folders = dict(sorted(date_folders.items(), key=lambda x: x[0]))        \n",
    "    \n",
    "    return list(date_folders.keys())\n",
    "\n",
    "def combine_folders_with_same_date(folders):\n",
    "    \"\"\"\n",
    "    Combines folders with the same date into one folder.\n",
    "\n",
    "    Args:\n",
    "        base (str): The base folder to search for days.\n",
    "    \"\"\"\n",
    "    primary_folder = folders[0]\n",
    "    folder_path = os.path.join(base, folders[1])\n",
    "    primary_folder_path = os.path.join(base, primary_folder)\n",
    "    \n",
    "    # Move contents to the primary folder\n",
    "    files=os.listdir(folder_path)\n",
    "        \n",
    "    pool=mp.Pool(mp.cpu_count())\n",
    "    pool.starmap(move_files, [(filename, folder_path, primary_folder_path) for filename in files])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Remove the now-empty folder\n",
    "    os.system(f\"rm -rf {folder_path}\")\n",
    "    print(f\"Combined {folders[1]} into {primary_folder}.\") \n",
    "\n",
    "def move_files(filename, folder_path, primary_folder_path):\n",
    "    \"\"\"\n",
    "    Moves a file from a folder to the primary folder.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The name of the file to move.\n",
    "        folder_path (str): The path to the folder containing the file.\n",
    "        primary_folder_path (str): The path to the primary folder.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.join(primary_folder_path, filename)): # Check if the file already exists in the primary folder\n",
    "        join=os.path.join(folder_path, filename)\n",
    "        shutil.move(join, primary_folder_path) # Move the file to the primary folder\n",
    "    # else:\n",
    "    #     print(\"already exists\")\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    \"\"\"\n",
    "    Extracts the timestamp from a DAS-h5-file's filename.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The filename to extract the timestamp from.\n",
    "        \n",
    "    Returns:\n",
    "        str: The timestamp part of the filename.\n",
    "    \"\"\"\n",
    "    # for the format is 'rhone1khz_UTC_yyyymmdd_hhmmss.ms.h5'\n",
    "    timestamp_part = filename.split('_')[2] + filename.split('_')[3]\n",
    "    return timestamp_part\n",
    "\n",
    "def get_filenames(folder, base):\n",
    "    \"\"\"\n",
    "    Collects the filenames in the data folder and sorts them by time.\n",
    "\n",
    "    Args:\n",
    "        folder (str): The folder to search for files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are integers and values are filenames, sorted chronologically.\n",
    "    \"\"\"\n",
    "    # Change to the folder directory\n",
    "    folder_path = os.path.join(base, folder)\n",
    "    os.chdir(folder_path)\n",
    "    files=os.listdir()\n",
    "    \n",
    "    # filter files to only get those where the filename contains 'rhone2das' or 'rhone1khz'\n",
    "    # Use fnmatch.filter to find matches for both patterns in a single pass\n",
    "    pattern_matches = fnmatch.filter(files, '*rhone2das*') + fnmatch.filter(files, '*rhone1khz*')\n",
    "    \n",
    "    # Sort the files by timestamp\n",
    "    sorted_files = sorted(pattern_matches, key=extract_timestamp)\n",
    "    \n",
    "    return  sorted_files \n",
    "\n",
    "def channel_fourier(data, args, taper, positions):\n",
    "    \"\"\"\n",
    "    Applies the Fourier transform to segments of the DAS records using the pyFFTW library.\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): The DAS data.\n",
    "        args (dict): The arguments for the Fourier transform.\n",
    "            Args requires the following keys:\n",
    "                \"fft_dtype\" (str): The data type for the Fourier transform.\n",
    "                \"num_frequency_points\" (int): The number of frequency points.\n",
    "                \"start_channel_index\" (int): The start channel index.\n",
    "                \"end_channel_index\" (int): The end channel index.\n",
    "                \"seg_len\" (int): The segment length.\n",
    "                \"hop\" (int): The hop size.\n",
    "                \"n_samples\" (int): The number of samples.\n",
    "        taper (np.array): The taper function.\n",
    "        positions (np.array): The positions of the segments in the data.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: The Fourier transformed segments.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack the arguments\n",
    "    seg_len = args[\"seg_len\"]\n",
    "    end_channel_index, start_channel_index = args[\"end_channel_index\"], args[\"start_channel_index\"]\n",
    "    fft_dtype = args[\"fft_dtype\"]  # dtype: float32\n",
    "    n_segments = positions.shape[0]\n",
    "\n",
    "    # Pre-allocate the segments array\n",
    "    Fsegs = np.zeros((n_segments, end_channel_index - start_channel_index, num_frequency_points), dtype=fft_dtype)  # empty float32 array\n",
    "    \n",
    "    # Pre-allocate the input array for FFTW\n",
    "    fft_input = pyfftw.empty_aligned(seg_len, dtype=fft_dtype)\n",
    "    # Create the FFTW object\n",
    "    fft_object = pyfftw.builders.rfft(fft_input)  # , planner_effort='FFTW_ESTIMATE') #, threads=mp.cpu_count()//2)\n",
    "    \n",
    "    # Prepare slices for efficient slicing\n",
    "    channel_slice = slice(start_channel_index, end_channel_index)\n",
    "    \n",
    "    for i, pos in enumerate(positions):\n",
    "        sliced_data = data[pos:pos + seg_len]\n",
    "        sliced_data = sliced_data.T[channel_slice].astype(fft_dtype)\n",
    "        \n",
    "        # Handle segments with varying lengths\n",
    "        if sliced_data.shape[1] != seg_len:\n",
    "            sliced_data = np.array([pad_or_truncate_channel(channel, seg_len) for channel in sliced_data])\n",
    "\n",
    "        # Compute the Fourier transform for each segment\n",
    "        for channel_number, channel in enumerate(sliced_data):\n",
    "            np.multiply(taper, channel, out=fft_input)  # Apply taper\n",
    "            fft_output = fft_object()  # Execute FFT\n",
    "            fourier_transformed = (10 * np.log(np.abs(fft_output) ** 2 + 1e-10))[:num_frequency_points]  # Compute power spectrum\n",
    "            fourier_transformed[0] = 0  # Remove DC component (average value of the signal)\n",
    "            Fsegs[i][channel_number] = fourier_transformed\n",
    "    \n",
    "    return Fsegs  # return the Fourier transformed segments\n",
    "\n",
    " \n",
    "\n",
    "def create_spectro_segment(file_index, args, filelist):\n",
    "    \"\"\"\n",
    "    Creates a spectrogram segment from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_index (int): The index of the file.\n",
    "        args (dict): The arguments for the Fourier transform.\n",
    "            Args requires the following keys:\n",
    "                \"n_files\" (int): The number of files.\n",
    "                \"seg_len\" (int): The segment length.\n",
    "                \"hop\" (int): The hop size.\n",
    "                \"n_samples\" (int): The number of samples.\n",
    "        filelist (list): The list of file names.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: The Fourier transformed segments.\n",
    "        int: The number of segments.\n",
    "    \"\"\"\n",
    "    \n",
    "    # chunk args\n",
    "    n_files=args[\"n_files\"]\n",
    "    seg_len=args[\"seg_len\"]\n",
    "    hop=args[\"hop\"]\n",
    "    n_samples=args[\"n_samples\"]\n",
    "    filename=filelist[file_index]\n",
    "    float_type=args[\"fft_dtype\"]\n",
    "    \n",
    "    #taper function\n",
    "    taper = signal.windows.tukey(seg_len, 0.25)  # reduces the amplitude of the discontinuities at the boundaries, thereby reducing spectral leakage.\n",
    "    \n",
    "    # Load the data\n",
    "    xr_h5=xr.open_dataset(filename, engine='h5netcdf', backend_kwargs={'phony_dims': 'access'})\n",
    "    data=xr_h5[\"Acoustic\"].compute().values.astype(float_type)\n",
    "        \n",
    "    # the windowing function (Tukey window in this case) tapers at the ends, \n",
    "    # to avoid losing data at the ends of each file, \n",
    "    # the end of one file is overlapped with the beginning of the next file.\n",
    "    if file_index!=n_files-1:\n",
    "        xr_h5_2=xr.open_dataset(filelist[file_index+1], engine='h5netcdf', backend_kwargs={'phony_dims': 'access'})\n",
    "        data_2=xr_h5_2[\"Acoustic\"].compute().values.astype(float_type)\n",
    "        \n",
    "        # Ensure the arrays have the same size along dimension 1\n",
    "        if data.shape[1:] == data_2.shape[1:]:\n",
    "            min_size = min(data.shape[1], data_2.shape[1])\n",
    "            data = data[:, :min_size]\n",
    "            data_2 = data_2[:, :min_size]\n",
    "        \n",
    "        data = np.concatenate((data, data_2[0:seg_len]), axis=0)\n",
    "    \n",
    "    next_file_index = file_index+1\n",
    "    file_pos = file_index * n_samples\n",
    "\n",
    "    # If the current file is not the last one\n",
    "    if file_index != n_files-1:\n",
    "        # Calculate the starting positions of each segment in the data\n",
    "        # first segment: (next_file_index-1)*n_samples/hop, rounded up\n",
    "        # last segment: (next_file_index*n_samples-1)/hop, rounded down\n",
    "        positions = np.arange(np.ceil((file_index)*n_samples/hop), np.floor((next_file_index*n_samples-1)/hop)+1, dtype=int)*hop - file_pos # scaled by the hop size and offset by the file position\n",
    "    else:\n",
    "        # If last one, start: (next_file_index*n_samples-seg_len)/hop\n",
    "        # to ensure that the last segment doesn't extend beyond the end of the data\n",
    "        positions = np.arange(np.ceil((file_index)*n_samples/hop), np.floor((next_file_index*n_samples-seg_len)/hop)+1, dtype=int)*hop - file_pos\n",
    "\n",
    "    start=time.time()\n",
    "    # Calculate the Fourier transformed segments\n",
    "    Fsegs = channel_fourier(data, args, taper, positions)\n",
    "    print(f\"Time taken for fft of {filename}: {time.time()-start}\")\n",
    "    \n",
    "    return Fsegs, positions.shape[0] # return the Fourier transformed segments and the number of segments\n",
    "\n",
    "\n",
    "def pad_or_truncate_channel(channel_data, seg_len):\n",
    "    \"\"\"\n",
    "    Pads or truncates the channel data to the correct segment length.\n",
    "    \n",
    "    Args:\n",
    "        channel_data (np.array): The channel data to pad or truncate.\n",
    "        seg_len (int): The segment length.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: The padded or truncated channel data.\n",
    "    \"\"\"\n",
    "    if channel_data.shape[0] < seg_len:\n",
    "        return np.pad(channel_data, (0, seg_len - channel_data.shape[0]), mode='constant')\n",
    "    else:\n",
    "        return channel_data[:seg_len]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########Base settings#########\n",
    "#granularity of spectrogram\n",
    "freq_res = 1 # frequency resolution in Hz\n",
    "time_res = 0.1 # time res in seconds\n",
    "float_type='float32'\n",
    "\n",
    "# section\n",
    "channel_distance = 4 # distance between channels in meters\n",
    "cable_start, cable_end = 0, 9200 # cable section to be processed (in meters) - 0==start\n",
    "start_channel_index, end_channel_index = cable_start // channel_distance, cable_end // channel_distance # channel distances to indices\n",
    "expected_channels = end_channel_index - start_channel_index # expected number of channels\n",
    "\n",
    "# Additional parameters:\n",
    "file_length = 30 # Length of a single h5 file in seconds.\n",
    "sample_freq = 1000 #Sampling frequency in Hz of the recorded data.\n",
    "freq_max = 100 # maximum frequency cut off value for the analysis\n",
    "seg_length=1/freq_res #calculate window length corresponding to freq_res\n",
    "n_samples = file_length*sample_freq #number of samples in one file/total number of data points available in one file\n",
    "num_frequency_points = int(seg_length*freq_max+1)\n",
    "seg_sample_len=int(seg_length*sample_freq) # how many time points should be in one processing window\n",
    "n_segments_file=int(2*(file_length/seg_length)) # amount of segments for the desired window length\n",
    "location_coords = np.arange(cable_start, cable_end, 4) # channel locations\n",
    "freq_coords=scipy.fft.rfftfreq(int(sample_freq/freq_res), 1/sample_freq)[:num_frequency_points] # frequency coordinates\n",
    "hop = int(time_res*sample_freq) # hop size - how many samples to skip between segments\n",
    "\n",
    "#fft input arguments\n",
    "args = {\n",
    "    \"fft_dtype\": float_type,\n",
    "    \"num_frequency_points\" : num_frequency_points,\n",
    "    \"start_channel_index\" : start_channel_index,\n",
    "    \"end_channel_index\" : end_channel_index,\n",
    "    \"seg_len\" : seg_sample_len,\n",
    "    \"hop\" : hop,\n",
    "    \"n_samples\" : n_samples,\n",
    "    \"seg_length\" : seg_length,\n",
    "    \"expected_channels\" : expected_channels\n",
    "}\n",
    "\n",
    "\n",
    "##########Main#########\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    # set the folder\n",
    "    folders=get_sorted_folders(base)\n",
    "    folder=folders[0]\n",
    "    \n",
    "    #path and name of resulting zarr-formatted data cube.\n",
    "    zarr_name = f\"cryo_cube_{folder}.zarr\"\n",
    "    zarr_path = f\"{zarr_base}/{zarr_name}\"\n",
    "    \n",
    "    os.chdir(base) # change to the base directory\n",
    "    print(f\"base folder: {os.getcwd()}\")\n",
    "    while os.path.exists(zarr_path) and folders:  # Check if folders is not empty\n",
    "        folder = folders.pop(0)  # remove and return the first element\n",
    "        zarr_name = f\"cryo_cube_{folder}.zarr\"\n",
    "        zarr_path = f\"{zarr_base}/{zarr_name}\"\n",
    "    \n",
    "    if not folders:\n",
    "        print(\"No more folders to process.\")\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        print(f\"Processing folder {folder}\")\n",
    "    \n",
    "    #get the day and month\n",
    "    day=folder[6:8]\n",
    "    month=folder[4:6]\n",
    "    \n",
    "    # print the settings\n",
    "    print(20*\"*\")\n",
    "    print(\"Max number of CPUs: \", total_cpus)\n",
    "    print(f\"Processed day: {day}.{month}.2020\")\n",
    "    print(f\"Time resolution: {time_res} sec\")\n",
    "    print(f\"Frequency resolution: {freq_res} Hz\")\n",
    "    print(f\"Resulting overlap: {1-hop/seg_sample_len}\")\n",
    "    print(f\"Expected number of channels: {expected_channels}\")\n",
    "    print(10*\"*\")\n",
    "    \n",
    "    # get the filenames and the total amount of segments\n",
    "    filenames = get_filenames(folder, base)\n",
    "    n_files=len(filenames)\n",
    "    args[\"n_files\"] = n_files\n",
    "    print(\"Number of files:\", n_files)\n",
    "    print(\"filenames\", filenames)\n",
    "    n_segments_total = int(np.floor((n_files*n_samples-seg_sample_len)/hop))+1 # total amount of segments\n",
    "\n",
    "    print(f\"Creating zarr shape...\")\n",
    "    # creating zarr shape\n",
    "    z_shape=(n_segments_total, expected_channels, num_frequency_points) \n",
    "    z_chunks=(n_segments_file,expected_channels,num_frequency_points)\n",
    "\n",
    "    print(\"Creating metadata...\")\n",
    "    start=time.time()\n",
    "\n",
    "    # Generate time coordinates based on the first file\n",
    "    dummy_file_path=os.path.join(base, folder, filenames[0])\n",
    "    dummy_xr = xr.open_dataset(filenames[0], engine='h5netcdf', backend_kwargs={'phony_dims': 'access'})\n",
    "    attr = dummy_xr['Acoustic'].attrs\n",
    "    start_time = np.datetime64(attr[\"ISO8601 Timestamp\"], 'ns') # Get the start time of the first file\n",
    "    time_res_ms = time_res * 1000  # Convert time_res from seconds to milliseconds\n",
    "    time_coords = start_time + np.arange(n_segments_total) * np.timedelta64(int(time_res_ms), 'ms') \n",
    "    \n",
    "    \n",
    "    fft_dask = da.zeros(z_shape, chunks=z_chunks, dtype=float_type) # create an empty dask array\n",
    "\n",
    "    xr_zarr = xr.Dataset(\n",
    "        {\n",
    "            \"fft\": ([\"time\", \"channel\", \"frequency\"], fft_dask),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": time_coords,\n",
    "            \"channel\": location_coords,\n",
    "            \"frequency\": freq_coords,\n",
    "        },\n",
    "    )\n",
    "    print(xr_zarr)\n",
    "        \n",
    "    print(f\"metadata created in {time.time()-start}s:\")\n",
    "\n",
    "    #xarray dataset to zarr\n",
    "    print(f\"Creating and writing empty {zarr_path} with metadata...\")\n",
    "    start=time.time()\n",
    "    xr_zarr.to_zarr(zarr_path, mode='w', consolidated=True)\n",
    "    print(f\"zarr created in {time.time()-start}s\")\n",
    "     \n",
    "    # In the following lines, multiple cpu-cores calculate\n",
    "    # a fft for each file simultanously.\n",
    "    # Before that we split the whole data to be processed in to not overload the memory!\n",
    "\n",
    "    # Determine available system memory \n",
    "    available_memory = psutil.virtual_memory().available * 0.8  # Use 80% of available memory (let's reserve some memory for the system and other processes)\n",
    "\n",
    "    # Calculate how many files can be processed simultaneously\n",
    "    memory_per_file = os.path.getsize(dummy_file_path)*1.5 # 1.5 times the size of the file, assuming some overhead\n",
    "    print(\"Memory per file (MB):\", memory_per_file / (1024**2))\n",
    "    files_at_once = int(available_memory / memory_per_file)\n",
    "    files_at_once = max(1, files_at_once) # Ensure that at least one file is processed at once\n",
    "\n",
    "    # Calculate the number of divisions \n",
    "    n_div = max(1, n_files // files_at_once)\n",
    "    index_list = np.arange(n_files)\n",
    "\n",
    "    # set split up\n",
    "    if n_files > files_at_once:\n",
    "        split_up = np.array_split(index_list, n_div)\n",
    "    else:\n",
    "        split_up = [index_list]\n",
    "\n",
    "    # Define the number of cores to be 90% of available cores, rounded down\n",
    "    n_cores = int(total_cpus * 0.9) // 1\n",
    "    print(\"Number of cores used:\", n_cores)\n",
    "\n",
    "    startT = time.time() # start the timer\n",
    "\n",
    "    running_index=0\n",
    "    for liste in split_up:\n",
    "        \n",
    "        print(f\"Starting FFT for split_up liste {liste}\")\n",
    "        # Start the local timer\n",
    "        start = time.time() \n",
    "        \n",
    "        # multiprocessing the fft calculation\n",
    "        pool=mp.Pool(n_cores)\n",
    "        fft_results=pool.starmap(create_spectro_segment, [(i, args, filenames) for i in liste])\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        # Print the time taken to process the files\n",
    "        end=time.time() # end the local timer\n",
    "        print(\"Time taken for fft from splitup\", end-start)\n",
    "        \n",
    "        fft_results = list(fft_results) # convert the map object to a list\n",
    "        \n",
    "        print(\"Writing liste to zarr...\")\n",
    "        start=time.time()\n",
    "        \n",
    "        # Convert results to Dask arrays and store them\n",
    "        for i in liste:\n",
    "            Fsegs, nseg = fft_results[i-int(liste[0])]\n",
    "            nseg = int(nseg)\n",
    "            dask_array = da.from_array(Fsegs, chunks=(nseg, expected_channels, num_frequency_points))\n",
    "            xr_zarr[\"fft\"][running_index:running_index+nseg] = dask_array\n",
    "            running_index += nseg\n",
    "            \n",
    "        xr_zarr.to_zarr(zarr_path, mode='a', consolidated=True)\n",
    "        print(f\"Wrote FFT to zarr using Dask in {time.time()-start}s for split_up {liste}\")\n",
    "        \n",
    "    print(20*\"*\")\n",
    "    print(\"Calculation completed\")\n",
    "    print(\"Total computation time in seconds:\", (-start_time+time.time())) \n",
    "    print(\"Computation time in seconds for fft:\", (-startT + time.time()))\n",
    "    print(\"Number of processed files:\", n_files)\n",
    "    print(\"Number of used cores:\", n_cores)\n",
    "    print(\"Time per File: \", ((-startT + time.time())/n_files))\n",
    "    print(20*\"*\")\n",
    "    \n",
    "    # submit the script again\n",
    "    if len(folders)>30: #\n",
    "        print(f\"Submitting the script again to process the next folder {folders[0]}\")\n",
    "        os.chdir(f\"{repo_dir}/code/slurm\")\n",
    "        os.system(f\"sbatch 02_fft_pipeline.sh\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rhoneCube",
   "language": "python",
   "name": "rhonecube"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
